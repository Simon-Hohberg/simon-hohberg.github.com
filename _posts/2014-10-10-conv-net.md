---
layout: post
title: Convolutional Neural Networks
---

A formal approach to convolutional neural networks.

<script type="text/javascript" src="/js/raphael-min.js"></script>
<script type="text/javascript" src="/js/jquery-2.1.1.min.js"></script>
<script type="text/javascript" src="/js/raphael-utils.js"></script>
<link rel="stylesheet" href="/css/svg.css">


### Convolution & (Cross-) Correlation

| | |
|----------:|--------|
| \\( i \\) | Image  |
| \\( k \\) | Kernel of size \\( N_k \times N_k \\) |

#### Convolution
<div class="math-definition">
$$
v\left(x,y\right) = \sum_{x_{k}=0}^{N_{k}-1}\sum_{y_{k}=0}^{N_{k}-1}i\left(x-x_{k},y-y_{k}\right)k\left(x_{k},y_{k}\right)
$$
</div>

#### (Cross-) Correlation
<div class="math-definition">
$$
l\left(x,y\right) = \sum_{x_{k}=0}^{N_{k}-1}\sum_{y_{k}=0}^{N_{k}-1}i\left(x+x_{k},y+y_{k}\right)k\left(x_{k},y_{k}\right)
$$
</div>


### Naming

| | |
|----------------------------------------:|--------------------------------------------------------------------------------------------------------------------------------|
|  \\( i_{m,n}^{l}\left(x,\, y\right) \\) | Input from feature map \\( m \\) at layer \\( l-1 \\) to feature map \\( n \\) at layer \\( l \\)                              |
|  \\( w_{m,n}^{l}\left(x,\, y\right) \\) | kernel (weights) for connection between feature map \\( m \\) at layer \\( l-1 \\) to feature map \\( n \\) at layer \\( l \\) |
|    \\( c_{n}^{l}\left(x,\, y\right) \\) | Complete input to feature map \\( n \\) at layer \\( l \\)                                                                     |
|                       \\( b_{n}^{l} \\) | Bias of feature map \\( n \\) at layer \\( l \\)                                                                               |
|                            \\( a(x) \\) | Activation function                                                                                                            |
|    \\( o_{n}^{l}\left(x,\, y\right) \\) | Output of feature map \\( n \\) at layer \\( l \\)                                                                             |
|                               \\( t \\) | Target (Label)                                                                                                                 |



### Feed-Forward

#### Single Input
<div class="math-definition">
$$ i_{n,m}^l\left(x,\, y \right) = \sum_{x',\, y'} o_{n}^{l-1}\left(x-x',\, y-y'\right) \cdot w_{n,m}^{l}\left(x',\, y' \right) $$
</div>

#### Complete Input
<div class="math-definition">
$$ \begin{aligned} c_{m}^l\left(x,\, y\right) \, & = \sum_{n} i_{n,m}^{l}\left(x,\,y\right) + b_{m}^{l} \\
  &= \sum_{n,\, x',\, y'} o_{n}^{l-1}\left(x-x',\, y-y'\right) \cdot w_{n,m}^{l}\left(x',\, y' \right) + b_{m}^{l} \end{aligned} $$
</div>

#### Output
<div class="math-definition">
$$ o_{m}^{l}\left(x,\, y\right) = a\left(c_{m}^{l}\left(x,\, y\right)\right)$$
</div>

#### Error
<div class="math-definition">
$$ E = \frac{1}{2} \left( t - o^L \right)^{2} $$
</div>


### Backpropagation

#### Weight Update
<div class="math-definition">
$$
\frac{\partial E}{\partial w_{n,m}^{l}\left(x,\, y\right)} = \sum_{x',y'}\underbrace{\frac{\partial E}{\partial c_{m}^{l}\left(x',\, y'\right)}}_{\delta_{m}^{l}\left(x',\, y' \right)} \cdot \frac{\partial c_{m}^{l}\left(x',\, y'\right)}{\partial w_{n,m}^{l}\left(x,\, y\right)}
$$
</div>

<div class="math-definition">
$$
\begin{aligned}
\frac{\partial c_{m}^{l}\left(x',\, y'\right)}{\partial w_{n,m}^{l}\left(x,\, y\right)} &= \frac{\partial}{\partial w_{n,m}^{l}\left(x,\, y\right)} \left( \sum_{n,\, x'',\, y''} o_{n}^{l-1}\left(x'-x'',\, y'-y''\right) \cdot w_{n,m}^{l}\left(x'',\, y'' \right) + b_{m}^{l} \right) \\
  &= \frac{\partial}{\partial w_{n,m}^{l}\left(x,\, y\right)} \left( o_{0}^{l-1}\left( x'-0,\, y'-0 \right) \cdot w_{0,m}^{l}\left( 0, \, 0 \right) + \ldots + o_{n}^{l-1}\left( x' - x,\, y' - y \right) \cdot w_{n,m}^{l} \left( x,\, y \right) + \ldots + b_{m}^{l} \right) \\
  &= o_{n}^{l-1}\left( x'-x,\, y' - y \right)
\end{aligned}
$$
</div>

<div class="math-definition">
$$
\frac{\partial E}{\partial w_{n,m}^{l}\left(x,\, y\right)} = \sum_{x',y'} o_{n}^{l-1}\left( x'-x,\, y' - y \right) \cdot \delta_{m}^{l}\left(x',\, y' \right)
$$
</div>

\\( \left( x'-x,\, y' - y \right) \\) will go out of bounds of \\( o\_{n}^{l-1} \\), 
since \\( x' \\) and \\( y' \\) run over the smaller \\( \delta\_{m}^{l} \\). We can resolve this by wrapping around the indexes, thus  rewriting \\( \left(x'-x,\,y'-y\right) \\) to \\( \left( N - x + x',\, N - y + y' \right)\\), where \\( N \\) is the size of \\( o_{n}^{l-1}\\). As a result we can apply cross-correlation, when rotating the result by 180°. We can rewrite the weight update as follows:

<div class="math-definition">
$$
\frac{\partial E}{\partial w_{n,m}^{l}\left(x,\, y\right)} = \operatorname{rot}_{180}\left(\underbrace{\sum_{x',y'} o_{n}^{l-1}\left( x + x',\, y + y' \right) \cdot \delta_{m}^{l}\left(x',\, y' \right)}_{\text{Cross-Correlation}}\right)
$$
</div>


#### Deltas
<div class="math-definition">
$$
\delta_{m}^{l}\left(x,\: y\right) = \frac{\partial E}{\partial c_{m}^{l}\left(x,\, y\right)}=\sum_{o,\, x',\, y'}\underbrace{\frac{\partial E}{\partial c_{o}^{l+1}\left(x',\, y'\right)}}_{\delta_{o}^{l+1}\left(x',\, y'\right)}\cdot\frac{\partial c_{o}^{l+1}\left(x',\, y'\right)}{c_{m}^{l}\left(x,\, y\right)}
$$
</div>

<div class="math-definition">
$$
\begin{aligned}

\frac{\partial c_{o}^{l+1}\left(x',\, y'\right)}{\partial c_{m}^{l}\left(x,\, y\right)} &= \frac{\partial}{\partial c_{m}^{l}\left(x,\, y\right)}\left(\sum_{m',\, x'',\, y''}w_{m',o}^{l+1}\left(x'',\, y''\right)\cdot a\left(c_{m'}^{l}\left(x'-x'',\, y'-y''\right)\right)\right) \\
  &= \frac{\partial}{\partial c_{m}^{l}\left(x,\, y\right)}\left(w_{0,o}^{l+1}\left(0,\,0\right)\cdot a\left(c_{0}^{l}\left(x'-0,\, y'-0\right)\right)+\ldots+w_{m,o}^{l+1}\left(x'',\, y''\right)\cdot a\left(c_{m}^{l}\underbrace{\left(x,\, y\right)}_{\left(x' - x'',\, y' - y''\right)}\right)+\ldots\right) \\
  &= w_{m,o}^{l+1}\left( x'-x,\, y'-y \right) \cdot \frac{\partial a\left( c_{m}^{l}\left(x,\, y\right) \right)}{\partial c_{m}^{l}\left(x,\, y\right)}
  
\end{aligned}
$$
</div>

<div class="math-definition">
$$
\delta_{m}^{l}\left(x,\: y\right) = \underbrace{\sum_{o,\, x',\, y'} w_{m,o}^{l+1}\left( x'-x,\, y'-y \right) \cdot \delta_{o}^{l+1}\left( x',\, y' \right)}_{\text{backpropagated error}} \cdot \frac{\partial a\left( c_{m}^{l}\left(x,\, y\right) \right)}{\partial c_{m}^{l}\left(x,\, y\right)}
$$
</div>

Thinking of \\( \left(x'-x,\, y'-y \right) \\) as \\( \left(N-x+x',\, N-y-y' \right) \\) we can again rewrite the delta recursion as cross-correlation where the result is rotated by 180°:

<div class="math-definition">
$$
\delta_{m}^{l}\left(x,\: y\right) = \operatorname{rot}_{180} \left(\underbrace{\sum_{o,\, x',\, y'} w_{m,o}^{l+1}\left( x+x',\, y+y' \right) \cdot \delta_{o}^{l+1}\left( x',\, y' \right)}_{\text{Cross-Correlation}} \right) \cdot \frac{\partial a\left( c_{m}^{l}\left(x,\, y\right) \right)}{\partial c_{m}^{l}\left(x,\, y\right)}
$$
</div>

<!--<div id="svg-container" class="svg-container"></div>-->

<!--<script type="text/javascript">-->
<!--  mathJaxRendered = function() {-->
<!--    var svg = $("svg");-->
<!--    addMathToSvg("mathJaxSource", svg);-->
<!--  }-->
<!--  -->
<!--  MathJax.Hub.Register.StartupHook("End Typeset", mathJaxRendered);-->
<!--  -->
<!--  var raphael = Raphael("svg-container", '100%', '100%');-->
<!--  raphael.setViewBox(0, 0, 800, 800, true);-->
<!--  -->
<!--  var grid = new Grid(10, 10, 8, 8, 10, 10);-->
<!--  -->
<!--  var neurons = [];-->
<!--  var links = [];-->
<!--  neurons.push(new Node(300, 50, 80, 20));-->
<!--  neurons.push(new Node(300, 150, 80, 20));-->
<!--  neurons.push(new Node(500, 50, 80, 20));-->
<!--  neurons.push(new Node(500, 150, 80, 20));-->
<!--  -->
<!--  links.push(neurons[0].linkTo(neurons[2]));-->
<!--  links.push(neurons[0].linkTo(neurons[3]));-->
<!--  links.push(neurons[1].linkTo(neurons[2]));-->
<!--  links.push(neurons[1].linkTo(neurons[3]));-->
<!--  -->
<!--  // draw-->
<!--  grid.draw(raphael);-->
<!--  for (var i = 0; i < neurons.length; i++) {-->
<!--    neurons[i].draw(raphael);-->
<!--  }-->
<!--  for (var i = 0; i < links.length; i++) {-->
<!--    links[i].draw(raphael);-->
<!--  }-->
<!--</script>-->
<!--<div id="mathJaxSource" style="display: none">-->
<!--  <math xmlns="http://www.w3.org/1998/Math/MathML"><mo>&#8721;</mo><mi>c</mi><mo>+</mo><mi>&#946;</mi></math>-->
<!--</div>-->

