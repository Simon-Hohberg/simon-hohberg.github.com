---
layout: post
title: Maxout Networks
---

Researching for my master thesis I tried to understand the paper by Goodfellow 
et al. on the *Maxout Units*. I found it very hard understanding the details
and thought a clear explanation in combination with a nice figure would be 
really helpful. So this is my shot at doing so.

__Please note, that everything explaned here was not developed by me, but is
just an explanation of the [paper by Goodfellow et al.](http://arxiv.org/abs/1302.4389)__

### Key infos about *Maxout*

- *Maxout* is an __activation function__
- supposed to be __combined with *dropout*__
- that __minimizes__ the model averaging __approximation error__ when using dropout
- is a __piecewise linear__ approximation to an arbitrary convex function

### Definition
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [["$","$"],["\\(","\\)"]]
    }
  });
</script>
<div style="padding: 5% 0;">
  <div style="float: left; width: 50%; padding: 10% 0">
    $$
    \begin{aligned}
    h_{i} \left( x \right) &= \max_{j\in\left[1,k\right]}\left(z_{ij}\right) \\
    z_{ij} &= x^{T} W_{\dots ij} + b_{ij} \\
    \end{aligned}
    $$
    $$
    W \in \mathbb{R}^{d\times m\times k}, \, b \in \mathbb{R}^{m\times k}
    $$
  </div>

  <table style="float: left; width: 45%; height: 5em">
    <thead>
    <tr>
      <th>&nbsp;</th>
      <th>&nbsp;</th>
    </tr>
    </thead>
    <tbody>
      <tr><td>$ h $</td><td><em>Maxout</em> function</td></tr>
      <tr><td>$ i $</td><td>index of neuron in the layer</td></tr>
      <tr><td>$ x $</td><td>input</td></tr>
      <tr><td>$ k $</td><td>number of linear pieces</td></tr>
      <tr><td>$ W $</td><td>4D tensor of learned weights</td></tr>
      <tr><td>$ b $</td><td>matrix of learned biases</td></tr>
    </tbody>
  </table>
</div>
<div style="clear: left"></div>

### Illustration

Now, here is how a single layer with three *Maxout* units looks like. Try 
hovering units with the mouse to better see the connection scheme.

<script type="text/javascript" src="/js/raphael/raphael-min.js"></script>
<script type="text/javascript" src="/js/raphael/raphael-utils.js"></script>
<script type="text/javascript" src="/js/raphael/net/nodes.js"></script>
<script type="text/javascript" src="/js/raphael/net/links.js"></script>
<script type="text/javascript" src="/js/raphael/net/net.js"></script>
<script type="text/javascript" src="/js/raphael/net/layers.js"></script>
<script type="text/javascript" src="/js/raphael/net/gradient-plugin.js"></script>

<div id="svg-container-0" class="svg-container"></div>

<script type="text/javascript">
  function net0() {
    var svgWidth = 700;
    var svgHeight = 320;
    var raphael = Raphael("svg-container-0", '100%', '100%');
    raphael.setViewBox(0, 0, svgWidth, svgHeight, true);
    //$("#svg-container").css("padding-bottom", ((svgHeight/svgWidth)*100) + "%")
    
    var black = "rgb(0,0,0)";
    var green = "rgb(0,255,0)";
    var blue = "rgb(0,0,255)"
    
    var net = new Net(raphael);
    var neuronRadius = 5;
    var inLayer = new FullyConnectedLayer(new Node(100, 70, neuronRadius, 0, black), 10, 20);
    var fcLayer1 = new FullyConnectedLayer(new Node(200, 110, neuronRadius, 0, green), 10, 20);
    var fcLayer2 = new FullyConnectedLayer(new Node(250, 70, neuronRadius, 0, green), 10, 20);
    var fcLayer3 = new FullyConnectedLayer(new Node(300, 30, neuronRadius, 0, green), 10, 20);
    var maxLayer = new FullyConnectedLayer(new Node(400, 70, neuronRadius, 0, blue), 10, 20);
    inLayer.link(fcLayer1);
    inLayer.link(fcLayer2);
    inLayer.link(fcLayer3);
    net.addLayer(inLayer).addLayer(fcLayer1).addLayer(fcLayer2).addLayer(fcLayer3).addLayer(maxLayer);
    for(var i = 0; i < maxLayer.nodes.length; i++) {
      net.link(i + 40, i + 10);
      net.link(i + 40, i + 20);
      net.link(i + 40, i + 30);
    }
    
    var inputNeuronIndexes = Array.apply(null, Array(10)).map(function (_, i) {return i;});
    var fcNeuronIndexes = Array.apply(null, Array(30)).map(function (_, i) {return i+10;});
    var maxPoolNeuronIndexes = Array.apply(null, Array(10)).map(function (_, i) {return i+40;});
    
    // draw
    net.draw(raphael);
    Array.apply(null, Array(10)).map(function (_, i) {return i+30;}).forEach(function(idx) {
      net.nodes[idx].nodeView.toBack();
    });

    var textX = raphael.text(100, 50, "x");
    textX.attr({ "font-size": 16, "font-family": "Arial, Helvetica, sans-serif", "fill": black });

    var textZ = raphael.text(300, 10, "z");
    textZ.attr({ "font-size": 16, "font-family": "Arial, Helvetica, sans-serif", "fill": black });
    
    var textH = raphael.text(400, 50, "h");
    textH.attr({ "font-size": 16, "font-family": "Arial, Helvetica, sans-serif", "fill": black });

    var textX = raphael.text(560, 40, "Input Units");
    textX.attr({ "font-size": 16, "font-family": "Arial, Helvetica, sans-serif" });
    var textFC = raphael.text(600, 60, "Fully-connected Units");
    textFC.attr({ "font-size": 16, "font-family": "Arial, Helvetica, sans-serif", "fill": green });
    var textFC = raphael.text(587, 80, "Max-pooling Units");
    textFC.attr({ "font-size": 16, "font-family": "Arial, Helvetica, sans-serif", "fill": blue });
    
    var inputNeuronExample = new Node(500, 40, neuronRadius, 0, black);
    inputNeuronExample.draw(raphael);
    var fcNeuronExample = new Node(500, 60, neuronRadius, 0, green);
    fcNeuronExample.draw(raphael);
    var maxPoolNeuronExample = new Node(500, 80, neuronRadius, 0, blue);
    maxPoolNeuronExample.draw(raphael);
    
    inputNeuronExample.nodeView.mouseover(function() {
      inputNeuronExample.highlight();
      net.highlight(inputNeuronIndexes);
    });
    inputNeuronExample.nodeView.mouseout(function() {
      inputNeuronExample.unhighlight();
      net.unhighlight(inputNeuronIndexes);
    });
    
    fcNeuronExample.nodeView.mouseover(function() {
      fcNeuronExample.highlight();
      net.highlight(fcNeuronIndexes);
    });
    fcNeuronExample.nodeView.mouseout(function() {
      fcNeuronExample.unhighlight();
      net.unhighlight(fcNeuronIndexes);
    });
    
    maxPoolNeuronExample.nodeView.mouseover(function() {
      maxPoolNeuronExample.highlight();
      net.highlight(maxPoolNeuronIndexes);
    });
    maxPoolNeuronExample.nodeView.mouseout(function() {
      maxPoolNeuronExample.unhighlight();
      net.unhighlight(maxPoolNeuronIndexes);
    });
  }
  net0();
</script>

*"But wait, this looks more like at least two layers!"*

Yes indeed, this is an important fact about *Maxout*. The activation function is
implemented using a small sub-network who's parameters are learned aswell 
(*"Did somebody say ['Network in Network'](http://arxiv.org/abs/1312.4400)?"*).
So a single *Maxout* layer actually consists of two parts (The first layer in 
the image is just the input layer. This doesn't necessarily mean that this is the
very first layer in the whole network, but can be the output of a previous layer, 
too).

The first is the linear part. It is a fully-connected layer with no activation 
function (which is referred to as *affine*), thus each unit in this layer just 
computes the weighted sum of all inputs, which is defined in the second part of
the *Maxout* definition above: 

$$x^{T} W_{\dots ij} + b_{ij}$$

Attentive readers might notice the missing biases in the image above. Well done.

The three-dimensional tensor $W$ contains the weights of this first part. The 
dots in the equation mean that all elements from the first dimension are taken. 
Consequently, $W_{\dots ij}$ is the weight vector of the unit in row $i$ and 
column $j$.

It might be also confusing, that in the figure above the units in this first 
part are aranged in a two-dimensional grid. The first dimension of this grid 
(number of rows) matches the number of input units, whereas the second 
dimension (number of columns) is the hyperparameter $k$, it is chosen when 
the whole architecture is defined. This parameter controls the complexity of 
the *Maxout* activation function. The higher the $k$ the more accurately any 
convex function can be approximated. Basically each column of units in this 
first part performs linear regression.

The second part is much easier. It is just doing max-pooling over each row of 
the first part, i.e. taking the maximum of the output of each row.

### An easy example

Consider the function $f\left(x\right)=x^{2}$.

We can approximate this function with a single *Maxout* unit that uses three 
linear pieces $k=3$. We can also say that the *Maxout* unit uses three hidden 
units.

This *Maxout* unit would look like this (biases included this time):

<div id="svg-container-1" class="svg-container"></div>

Each hidden unit calculates:

$$ z_{j} = x \cdot w_{j} + b_{j} $$ 

This is a simple linear function. The max-pooling unit takes the 
maximum of these three linear functions.

Take a look at this picture. It shows the $x^{2}$ function and three linear 
functions that could be learned by the *Maxout* unit.

<img style="display: block; width: 30em; margin: 0.5em auto 0.5em auto" src="{{ site.url }}/assets/approximation.svg" alt="Approximation using three linear functions">

Finally, try imagining how this would look like with 4, 5, 6 or an arbitrary 
number of linear functions.

<script type="text/javascript">
  function net1() {
    var svgWidth = 700;
    var svgHeight = 160;
    var raphael = Raphael("svg-container-1", '100%', '100%');
    raphael.setViewBox(0, 0, svgWidth, svgHeight, true);
    //$("#svg-container").css("padding-bottom", ((svgHeight/svgWidth)*100) + "%")
    
    var black = "rgb(0,0,0)";
    var grey = "rgb(180,180,180)";
    var green = "rgb(0,255,0)";
    var blue = "rgb(0,0,255)"
    
    var net = new Net(raphael);
    var neuronRadius = 5;
    var inLayer = new FullyConnectedLayer(new Node(100, 70, neuronRadius, 0, black), 1, 20);
    var fcLayer1 = new FullyConnectedLayer(new Node(200, 110, neuronRadius, 0, green), 1, 20);
    var fcLayer2 = new FullyConnectedLayer(new Node(250, 70, neuronRadius, 0, green), 1, 20);
    var fcLayer3 = new FullyConnectedLayer(new Node(300, 30, neuronRadius, 0, green), 1, 20);
    var maxLayer = new FullyConnectedLayer(new Node(400, 70, neuronRadius, 0, blue), 1, 20);
    inLayer.link(fcLayer1);
    inLayer.link(fcLayer2);
    inLayer.link(fcLayer3);
    net.addLayer(inLayer).addLayer(fcLayer1).addLayer(fcLayer2).addLayer(fcLayer3).addLayer(maxLayer);
    for(var i = 0; i < maxLayer.nodes.length; i++) {
      net.link(i + 4, i + 1);
      net.link(i + 4, i + 2);
      net.link(i + 4, i + 3);
    }
    var bias1 = new Node(120, 140, neuronRadius, 0, grey);
    var bias2 = new Node(170, 100, neuronRadius, 0, grey);
    var bias3 = new Node(220, 60, neuronRadius, 0, grey);
    
    net.addNode(bias1).addNode(bias2).addNode(bias3);
    net.link(1, 5).link(2, 6).link(3, 7);
    
    var inputNeuronIndexes = [0];
    var fcNeuronIndexes = [1,2,3]
    var maxPoolNeuronIndexes = [4];
    var biasIndexes = [5,6,7];
    
    // draw
    net.draw(raphael);
    net.nodes[6].nodeView.toBack();

    var textX = raphael.text(100, 50, "x");
    textX.attr({ "font-size": 16, "font-family": "Arial, Helvetica, sans-serif", "fill": black });

    var textZ = raphael.text(300, 10, "z");
    textZ.attr({ "font-size": 16, "font-family": "Arial, Helvetica, sans-serif", "fill": black });
    
    var textH = raphael.text(400, 50, "h");
    textH.attr({ "font-size": 16, "font-family": "Arial, Helvetica, sans-serif", "fill": black });

    var textX = raphael.text(560, 20, "Input Units");
    textX.attr({ "font-size": 16, "font-family": "Arial, Helvetica, sans-serif" });
    var textFC = raphael.text(600, 40, "Fully-connected Units");
    textFC.attr({ "font-size": 16, "font-family": "Arial, Helvetica, sans-serif", "fill": green });
    var textFC = raphael.text(587, 60, "Max-pooling Units");
    textFC.attr({ "font-size": 16, "font-family": "Arial, Helvetica, sans-serif", "fill": blue });
    var textBias = raphael.text(546, 80, "Biases");
    textBias.attr({ "font-size": 16, "font-family": "Arial, Helvetica, sans-serif", "fill": grey });
    
    var inputNeuronExample = new Node(500, 20, neuronRadius, 0, black);
    inputNeuronExample.draw(raphael);
    var fcNeuronExample = new Node(500, 40, neuronRadius, 0, green);
    fcNeuronExample.draw(raphael);
    var maxPoolNeuronExample = new Node(500, 60, neuronRadius, 0, blue);
    maxPoolNeuronExample.draw(raphael);
    var biasExample = new Node(500, 80, neuronRadius, 0, grey);
    biasExample.draw(raphael);
    
    inputNeuronExample.nodeView.mouseover(function() {
      inputNeuronExample.highlight();
      net.highlight(inputNeuronIndexes);
    });
    inputNeuronExample.nodeView.mouseout(function() {
      inputNeuronExample.unhighlight();
      net.unhighlight(inputNeuronIndexes);
    });
    
    fcNeuronExample.nodeView.mouseover(function() {
      fcNeuronExample.highlight();
      net.highlight(fcNeuronIndexes);
    });
    fcNeuronExample.nodeView.mouseout(function() {
      fcNeuronExample.unhighlight();
      net.unhighlight(fcNeuronIndexes);
    });
    
    maxPoolNeuronExample.nodeView.mouseover(function() {
      maxPoolNeuronExample.highlight();
      net.highlight(maxPoolNeuronIndexes);
    });
    maxPoolNeuronExample.nodeView.mouseout(function() {
      maxPoolNeuronExample.unhighlight();
      net.unhighlight(maxPoolNeuronIndexes);
    });
    
    biasExample.nodeView.mouseover(function() {
      biasExample.highlight();
      net.highlight(biasIndexes);
    });
    biasExample.nodeView.mouseout(function() {
      biasExample.unhighlight();
      net.unhighlight(biasIndexes);
    });
  }
  net1();
</script>