---
layout: post
title: Maxout Networks
---

Researching for my master thesis I tried to understand the paper by Goodfellow 
et al. on the *Maxout Units*. I found it very hard understanding the details
and thought a clear explanation in combination with a nice figure would be 
really helpful. So this is my shot at doing so.

__Please note, that everything explaned here was not developed by me, but is
just an explanation of the [paper by Goodfellow et al.](http://arxiv.org/abs/1302.4389)__

### Key infos about *Maxout*

- *Maxout* is an __activation function__
- supposed to be __combined with *dropout*__
- that __minimizes__ the model averaging __approximation error__ when using dropout
- is a __piecewise linear__ approximation to an arbitrary convex function

### Definition
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [["$","$"],["\\(","\\)"]]
    }
  });
</script>
<div style="padding: 5% 0;">
  <div style="float: left; width: 40%; padding: 10% 0">
    $$
    \begin{aligned}
    h_{i} \left( x \right) &= \max_{j\in\left[1,k\right]}\left(z_{ij}\right) \\
    z_{ij} &= x^{T} W_{\dots ij} + b_{ij} \\
    \end{aligned}
    $$
  </div>

  <table style="float: left; height: 5em">
    <thead>
    <tr>
      <th>&nbsp;</th>
      <th>&nbsp;</th>
    </tr>
    </thead>
    <tbody>
      <tr><td>$ h $</td><td><em>Maxout</em> function</td></tr>
      <tr><td>$ x $</td><td>Input ($\in \mathbb{R}^{d}$)</td></tr>
      <tr><td>$ W $</td><td>4D tensor of learned weights ($\in \mathbb{R}^{d\times m \times k}$)</td></tr>
      <tr><td>$ d $</td><td>Number of input units (length of x)</td></tr>
      <tr><td>$ m $</td><td>Number of units in each linear feature extractor (complexity)</td></tr>
      <tr><td>$ k $</td><td>Number of linear feature extractors</td></tr>
      <tr><td>$ b $</td><td>Matrix of learned biases ($\in \mathbb{R}^{m\times k}$)</td></tr>
      <tr><td>$ i $</td><td>Runs over the number of <em>Maxout</em> units ($\in \left[1,m \right]$)</td></tr>
      <tr><td>$ j $</td><td>Runs over the number of feature extractors ($\in \left[1,k \right]$)</td></tr>
    </tbody>
  </table>
</div>
<div style="clear: left"></div>

### Illustration

Now, here is how a single layer with five *Maxout* units and three hidden linear 
feature extractors looks like. Try hovering units with the mouse to better see 
the connection scheme.

<script type="text/javascript" src="/js/raphael/raphael-min.js"></script>
<script type="text/javascript" src="/js/raphael/raphael-utils.js"></script>
<script type="text/javascript" src="/js/raphael/net/nodes.js"></script>
<script type="text/javascript" src="/js/raphael/net/links.js"></script>
<script type="text/javascript" src="/js/raphael/net/net.js"></script>
<script type="text/javascript" src="/js/raphael/net/layers.js"></script>
<script type="text/javascript" src="/js/raphael/net/gradient-plugin.js"></script>

<div id="svg-container-0" class="svg-container"></div>

<script type="text/javascript">
  Raphael.fn.arrow = function(x1, y1, x2, y2, size) {
    var angle = Raphael.angle(x1, y1, x2, y2);
    var a45   = Raphael.rad(angle-45);
    var a45m  = Raphael.rad(angle+45);
    var a135  = Raphael.rad(angle-135);
    var a135m = Raphael.rad(angle+135);
    var x1a = x1 + Math.cos(a135) * size;
    var y1a = y1 + Math.sin(a135) * size;
    var x1b = x1 + Math.cos(a135m) * size;
    var y1b = y1 + Math.sin(a135m) * size;
    var x2a = x2 + Math.cos(a45) * size;
    var y2a = y2 + Math.sin(a45) * size;
    var x2b = x2 + Math.cos(a45m) * size;
    var y2b = y2 + Math.sin(a45m) * size;
    return this.path(
//      "M"+x1+" "+y1+"L"+x1a+" "+y1a+
//      "M"+x1+" "+y1+"L"+x1b+" "+y1b+
      "M"+x1+" "+y1+"L"+x2+" "+y2+
      "M"+x2+" "+y2+"L"+x2a+" "+y2a+
      "M"+x2+" "+y2+"L"+x2b+" "+y2b
    );
  };
  function net0() {
    var svgWidth = 700;
    var svgHeight = 260;
    var raphael = Raphael("svg-container-0", '100%', '100%');
    raphael.setViewBox(0, 0, svgWidth, svgHeight, true);
    //$("#svg-container").css("padding-bottom", ((svgHeight/svgWidth)*100) + "%")
    
    var black = "rgb(0,0,0)";
    var green = "rgb(0,255,0)";
    var blue = "rgb(0,0,255)"
    var grey = "rgb(180,180,180)";
    var darkGrey = "rgb(100,100,100)";
    
    var numMaxoutUnits = 5;
    
    var net = new Net(raphael);
    var neuronRadius = 5;
    var inLayer = new FullyConnectedLayer(new Node(100, 70, neuronRadius, 0, black), numMaxoutUnits, 30);
    var fcLayer1 = new FullyConnectedLayer(new Node(200, 110, neuronRadius, 0, green), numMaxoutUnits, 30);
    var fcLayer2 = new FullyConnectedLayer(new Node(250, 70, neuronRadius, 0, green), numMaxoutUnits, 30);
    var fcLayer3 = new FullyConnectedLayer(new Node(300, 30, neuronRadius, 0, green), numMaxoutUnits, 30);
    var maxLayer = new FullyConnectedLayer(new Node(400, 70, neuronRadius, 0, blue), numMaxoutUnits, 30);
    inLayer.link(fcLayer1);
    inLayer.link(fcLayer2);
    inLayer.link(fcLayer3);
    net.addLayer(inLayer).addLayer(fcLayer1).addLayer(fcLayer2).addLayer(fcLayer3).addLayer(maxLayer);
    for(var i = 0; i < maxLayer.nodes.length; i++) {
      net.link(i + 4*numMaxoutUnits, i + numMaxoutUnits);
      net.link(i + 4*numMaxoutUnits, i + 2*numMaxoutUnits);
      net.link(i + 4*numMaxoutUnits, i + 3*numMaxoutUnits);
    }
    var bias = new FullyConnectedLayer(new Node(100, 220, neuronRadius, 0, grey), 1, 0);
    
    bias.link(fcLayer1);
    bias.link(fcLayer2);
    bias.link(fcLayer3);
    net.addLayer(bias);
    
    var inputNeuronIndexes = Array.apply(null, Array(numMaxoutUnits)).map(function (_, i) {return i;});
    var fcNeuronIndexes = Array.apply(null, Array(3*numMaxoutUnits)).map(function (_, i) {return i+numMaxoutUnits;});
    var maxPoolNeuronIndexes = Array.apply(null, Array(numMaxoutUnits)).map(function (_, i) {return i+4*numMaxoutUnits;});
    var biasIndexes = [5*numMaxoutUnits]
    
    // draw
    net.draw(raphael);
    Array.apply(null, Array(numMaxoutUnits)).map(function (_, i) {return i+3*numMaxoutUnits;}).forEach(function(idx) {
      net.nodes[idx].nodeView.toBack();
    });
    
    // indexes
    var dArrow = raphael.arrow(80, 70, 80, 190, 3).toBack();
    dArrow.attr({"stroke": darkGrey});
    var textD = raphael.text(70, 80, "d");
    textD.attr({ "font-size": 16, "font-family": "Arial, Helvetica, sans-serif", "fill": darkGrey });
    
    var kArrow = raphael.arrow(200, 250, 300, 170, 3).toBack();
    kArrow.attr({"stroke": darkGrey});
    var textK = raphael.text(230, 240, "k");
    textK.attr({ "font-size": 16, "font-family": "Arial, Helvetica, sans-serif", "fill": darkGrey });
    
    var mArrow = raphael.arrow(320, 30, 320, 150, 3).toBack();
    mArrow.attr({"stroke": darkGrey});
    var textM = raphael.text(330, 40, "m");
    textM.attr({ "font-size": 16, "font-family": "Arial, Helvetica, sans-serif", "fill": darkGrey });
    
    var mArrow = raphael.arrow(420, 70, 420, 190, 3).toBack();
    mArrow.attr({"stroke": darkGrey});
    var textM = raphael.text(430, 80, "m");
    textM.attr({ "font-size": 16, "font-family": "Arial, Helvetica, sans-serif", "fill": darkGrey });


    var textX = raphael.text(100, 50, "x");
    textX.attr({ "font-size": 16, "font-family": "Arial, Helvetica, sans-serif", "fill": black });

    var textZ = raphael.text(300, 10, "z");
    textZ.attr({ "font-size": 16, "font-family": "Arial, Helvetica, sans-serif", "fill": black });
    
    var textH = raphael.text(400, 50, "h");
    textH.attr({ "font-size": 16, "font-family": "Arial, Helvetica, sans-serif", "fill": black });

    var textX = raphael.text(560, 20, "Input Units");
    textX.attr({ "font-size": 16, "font-family": "Arial, Helvetica, sans-serif" });
    var textFC = raphael.text(600, 40, "Fully-connected Units");
    textFC.attr({ "font-size": 16, "font-family": "Arial, Helvetica, sans-serif", "fill": green });
    var textFC = raphael.text(587, 60, "Max-pooling Units");
    textFC.attr({ "font-size": 16, "font-family": "Arial, Helvetica, sans-serif", "fill": blue });
    var textBias = raphael.text(538, 80, "Bias");
    textBias.attr({ "font-size": 16, "font-family": "Arial, Helvetica, sans-serif", "fill": grey });
    
    var inputNeuronExample = new Node(500, 20, neuronRadius, 0, black);
    inputNeuronExample.draw(raphael);
    var fcNeuronExample = new Node(500, 40, neuronRadius, 0, green);
    fcNeuronExample.draw(raphael);
    var maxPoolNeuronExample = new Node(500, 60, neuronRadius, 0, blue);
    maxPoolNeuronExample.draw(raphael);
    var biasExample = new Node(500, 80, neuronRadius, 0, grey);
    biasExample.draw(raphael);
    
    inputNeuronExample.nodeView.mouseover(function() {
      inputNeuronExample.highlight();
      net.highlight(inputNeuronIndexes);
    });
    inputNeuronExample.nodeView.mouseout(function() {
      inputNeuronExample.unhighlight();
      net.unhighlight(inputNeuronIndexes);
    });
    
    fcNeuronExample.nodeView.mouseover(function() {
      fcNeuronExample.highlight();
      net.highlight(fcNeuronIndexes);
    });
    fcNeuronExample.nodeView.mouseout(function() {
      fcNeuronExample.unhighlight();
      net.unhighlight(fcNeuronIndexes);
    });
    
    maxPoolNeuronExample.nodeView.mouseover(function() {
      maxPoolNeuronExample.highlight();
      net.highlight(maxPoolNeuronIndexes);
    });
    maxPoolNeuronExample.nodeView.mouseout(function() {
      maxPoolNeuronExample.unhighlight();
      net.unhighlight(maxPoolNeuronIndexes);
    });
    
    biasExample.nodeView.mouseover(function() {
      biasExample.highlight();
      net.highlight(biasIndexes);
    });
    biasExample.nodeView.mouseout(function() {
      biasExample.unhighlight();
      net.unhighlight(biasIndexes);
    });
  }
  net0();
</script>

*"But wait, this looks more like at least two layers!"*

Yes indeed, this is very important and probably confusing about *Maxout*. The 
activation function is implemented using a small sub-network who's parameters 
are learned aswell (*"Did somebody say 
['Network in Network'](http://arxiv.org/abs/1312.4400)?"*).

So if we don't count the input layer, a single layer of *Maxout* units 
actually consists of two layers itself (Although I referred to the first layer 
as input layer, this doesn't necessarily mean that it is the
very first layer in the whole network, but can be the output of a previous layer, 
too).

Let's call the first layer the *hidden* layer. It implements the linear part of 
the *Maxout* units. It is a set of fully-connected layers (the columns in the image) 
with no activation function (which is referred to as *affine*), thus each unit 
in this layer just computes the weighted sum of all inputs, which is defined in 
the second part of the *Maxout* definition above: 

$$x^{T} W_{\dots ij} + b_{ij}$$

Don't get confused about the biases. In this definition they form a matrix, 
however usually biases are implicit by just adding an additional 1 to the inputs,
so that the weight matrix is slightly bigger than for the regular inputs only.
So actually the bias is an additional weight in the matrix $W$. Maybe think of
the bias matrix in this definition as the slice of weights in the weight matrix.

Now the three-dimensional tensor $W$ contains the weights of this first part. The 
dots in the equation mean that all elements from the first dimension are taken 
like `W[:, i, j]` in Python or `W(:, i, j)` in Matlab. 
Consequently, $W_{\dots ij}$ is the weight vector of the unit in row $i$ and 
column $j$.

In the figure above the units in this first part are aranged in a two-dimensional 
grid. The first dimension of this grid (number of rows) doesn't have to matches 
the number of input units, both $j$ and the second dimension $k$ 
(number of columns) are hyperparameters, which are chosen when 
the whole architecture is defined. These two parameters control the complexity of 
the *Maxout* activation function. The higher the $k$ and $j$ the more accurately
 any convex function can be approximated. Basically each column of units in this 
first part performs linear regression.

The second part is much easier. It is just doing max-pooling over each row of 
the first part, i.e. taking the maximum of the output along each row.

### A simple example

Consider the function $f\left(x\right)=x^{2}$.

We can approximate this function with a single *Maxout* unit that uses three 
linear pieces $k=3$. So it uses three hidden units.

This *Maxout* unit would look like this (biases included this time):

<div id="svg-container-1" class="svg-container"></div>

Each hidden unit calculates:

$$ z_{j} = x \cdot w_{j} + b_{j} $$ 

This is a simple linear function. The max-pooling unit takes the 
maximum of these three linear functions.

Take a look at this picture. It shows the $x^{2}$ function and three linear 
functions that could be learned by the *Maxout* unit.

<img style="display: block; width: 30em; margin: 0.5em auto 0.5em auto" src="{{ site.url }}/assets/approximation.svg" alt="Approximation using three linear functions">

Finally, try imagining how this would look like with 4, 5, 6 or an arbitrary 
number of linear functions. That's right, it would be a nice approximation that
is linear everywhere, except for the connection points of the linear parts.

<script type="text/javascript">
  function net1() {
    var svgWidth = 700;
    var svgHeight = 160;
    var raphael = Raphael("svg-container-1", '100%', '100%');
    raphael.setViewBox(0, 0, svgWidth, svgHeight, true);
    //$("#svg-container").css("padding-bottom", ((svgHeight/svgWidth)*100) + "%")
    
    var black = "rgb(0,0,0)";
    var grey = "rgb(180,180,180)";
    var green = "rgb(0,255,0)";
    var blue = "rgb(0,0,255)"
    
    var net = new Net(raphael);
    var neuronRadius = 5;
    var inLayer = new FullyConnectedLayer(new Node(100, 70, neuronRadius, 0, black), 1, 20);
    var fcLayer1 = new FullyConnectedLayer(new Node(200, 110, neuronRadius, 0, green), 1, 20);
    var fcLayer2 = new FullyConnectedLayer(new Node(250, 70, neuronRadius, 0, green), 1, 20);
    var fcLayer3 = new FullyConnectedLayer(new Node(300, 30, neuronRadius, 0, green), 1, 20);
    var maxLayer = new FullyConnectedLayer(new Node(400, 70, neuronRadius, 0, blue), 1, 20);
    inLayer.link(fcLayer1);
    inLayer.link(fcLayer2);
    inLayer.link(fcLayer3);
    net.addLayer(inLayer).addLayer(fcLayer1).addLayer(fcLayer2).addLayer(fcLayer3).addLayer(maxLayer);
    for(var i = 0; i < maxLayer.nodes.length; i++) {
      net.link(i + 4, i + 1);
      net.link(i + 4, i + 2);
      net.link(i + 4, i + 3);
    }
    var bias = new FullyConnectedLayer(new Node(100, 100, neuronRadius, 0, grey), 1, 0);
    
    bias.link(fcLayer1);
    bias.link(fcLayer2);
    bias.link(fcLayer3);
    net.addLayer(bias);
    
    var inputNeuronIndexes = [0];
    var fcNeuronIndexes = [1,2,3]
    var maxPoolNeuronIndexes = [4];
    var biasIndexes = [5];
    
    // draw
    net.draw(raphael);

    var textX = raphael.text(100, 50, "x");
    textX.attr({ "font-size": 16, "font-family": "Arial, Helvetica, sans-serif", "fill": black });

    var textZ = raphael.text(300, 10, "z");
    textZ.attr({ "font-size": 16, "font-family": "Arial, Helvetica, sans-serif", "fill": black });
    
    var textH = raphael.text(400, 50, "h");
    textH.attr({ "font-size": 16, "font-family": "Arial, Helvetica, sans-serif", "fill": black });

    var textX = raphael.text(560, 20, "Input Units");
    textX.attr({ "font-size": 16, "font-family": "Arial, Helvetica, sans-serif" });
    var textFC = raphael.text(600, 40, "Fully-connected Units");
    textFC.attr({ "font-size": 16, "font-family": "Arial, Helvetica, sans-serif", "fill": green });
    var textFC = raphael.text(587, 60, "Max-pooling Units");
    textFC.attr({ "font-size": 16, "font-family": "Arial, Helvetica, sans-serif", "fill": blue });
    var textBias = raphael.text(538, 80, "Bias");
    textBias.attr({ "font-size": 16, "font-family": "Arial, Helvetica, sans-serif", "fill": grey });
    
    var inputNeuronExample = new Node(500, 20, neuronRadius, 0, black);
    inputNeuronExample.draw(raphael);
    var fcNeuronExample = new Node(500, 40, neuronRadius, 0, green);
    fcNeuronExample.draw(raphael);
    var maxPoolNeuronExample = new Node(500, 60, neuronRadius, 0, blue);
    maxPoolNeuronExample.draw(raphael);
    var biasExample = new Node(500, 80, neuronRadius, 0, grey);
    biasExample.draw(raphael);
    
    inputNeuronExample.nodeView.mouseover(function() {
      inputNeuronExample.highlight();
      net.highlight(inputNeuronIndexes);
    });
    inputNeuronExample.nodeView.mouseout(function() {
      inputNeuronExample.unhighlight();
      net.unhighlight(inputNeuronIndexes);
    });
    
    fcNeuronExample.nodeView.mouseover(function() {
      fcNeuronExample.highlight();
      net.highlight(fcNeuronIndexes);
    });
    fcNeuronExample.nodeView.mouseout(function() {
      fcNeuronExample.unhighlight();
      net.unhighlight(fcNeuronIndexes);
    });
    
    maxPoolNeuronExample.nodeView.mouseover(function() {
      maxPoolNeuronExample.highlight();
      net.highlight(maxPoolNeuronIndexes);
    });
    maxPoolNeuronExample.nodeView.mouseout(function() {
      maxPoolNeuronExample.unhighlight();
      net.unhighlight(maxPoolNeuronIndexes);
    });
    
    biasExample.nodeView.mouseover(function() {
      biasExample.highlight();
      net.highlight(biasIndexes);
    });
    biasExample.nodeView.mouseout(function() {
      biasExample.unhighlight();
      net.unhighlight(biasIndexes);
    });
  }
  net1();
</script>

### Where is *Dropout* in all this?

*Dropout* is a regularization mechanism. It simulates the training of a bag of
networks with different architectures. However, this bag of networks contains
only those networks that can be created by dropping an arbitrary number of 
weights from the original network. In practice this is implemented by randomly
dropping connections between neurons with a certain probability during training.
As a result in each training pass a kind of different network is trained, however
-- and this is important -- all networks share the same weights, because we are
acutally using only a single network and just use the weights of that all the 
time.

When applying *bagging*, i.e. using $n$ models for prediction rather than just
one, it is necessary to combine the predictions of all models, which can be
easily done by calculating the mean of all predictions. In the *dropout* case
this can not be done, because we actually have only one model. (Ok, it could be
done but it doesn't make sense, since the number of models is way too big). 
Instead, a much better approach is to just scale the weights the whole network
proportional to the drop ratio. The issue then is, that this is only accurate
for a single linear layer with the *Softmax* function applied. Thus, for deeper
models that use a non-linear activation function it is not accurate anymore.

And that is why *Maxout* works as we have seen above. Dropout can be applied to 
the first part of *Maxout* and model averaging is still accurate, because there
are no non-linearities involved. Pretty clever.
