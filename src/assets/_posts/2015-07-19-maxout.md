---
layout: post
title: Maxout Networks
keywords: CNN, Maxout, Dropout, Goodfellow
thumbImg: ./assets/imgs/maxout.svg
thumbColor: #947A8B
---

# Maxout Networks

Researching for my master thesis I tried to understand the paper by Goodfellow 
et al. on the *Maxout Units*. I found it very hard understanding the details
and thought a clear explanation in combination with a nice figure would be 
really helpful. So this is my shot at doing so.

__Please note, that everything explaned here was not developed by me, but is
just an explanation of the [paper by Goodfellow et al.](http://arxiv.org/abs/1302.4389)__

### Key infos about *Maxout*

- *Maxout* is an __activation function__  
- supposed to be __combined with *dropout*__  
- that <b>minimizes</b> the model averaging __approximation error__ when using dropout  
- test
- is a __piecewise linear__ approximation to an arbitrary convex function

### Definition

<div style="padding: 5% 0;">
  <div style="float: left; width: 40%; padding: 10% 0">
    $$
    \begin{aligned}
    h_{i} \left( x \right) &= \max_{j\in\left[1,k\right]}\left(z_{ij}\right) \\
    z_{ij} &= x^{T} W_{\dots ij} + b_{ij} \\
    \end{aligned}
    $$
  </div>

  <table style="float: left; height: 5em">
    <thead>
    <tr>
      <th>&nbsp;</th>
      <th>&nbsp;</th>
    </tr>
    </thead>
    <tbody>
      <tr><td>\( h \)</td><td><em>Maxout</em> function</td></tr>
      <tr><td>\( x \)</td><td>Input (\(\in \mathbb{R}^{d}\))</td></tr>
      <tr><td>\( W \)</td><td>4D tensor of learned weights (\(\in \mathbb{R}^{d\times m \times k}\))</td></tr>
      <tr><td>\( d \)</td><td>Number of input units (length of x)</td></tr>
      <tr><td>\( m \)</td><td>Number of units in each linear feature extractor (complexity)</td></tr>
      <tr><td>\( k \)</td><td>Number of linear feature extractors</td></tr>
      <tr><td>\( b \)</td><td>Matrix of learned biases (\(\in \mathbb{R}^{m\times k}\))</td></tr>
      <tr><td>\( i \)</td><td>Runs over the number of <em>Maxout</em> units (\(\in \left[1,m \right]\))</td></tr>
      <tr><td>\( j \)</td><td>Runs over the number of feature extractors (\(\in \left[1,k \right]\))</td></tr>
    </tbody>
  </table>
</div>
<div style="clear: left"></div>

### Illustration

Now, here is how a single layer with five *Maxout* units and three hidden linear 
feature extractors looks like. Try hovering units with the mouse to better see 
the connection scheme.

<div id="svg-container-0" class="svg-container"></div>

*"But wait, this looks more like at least two layers!"*

Yes indeed, this is very important and probably confusing about *Maxout*. The 
activation function is implemented using a small sub-network who's parameters 
are learned aswell (*"Did somebody say 
['Network in Network'](http://arxiv.org/abs/1312.4400)?"*).

So if we don't count the input layer, a single layer of *Maxout* units 
actually consists of two layers itself (Although I referred to the first layer 
as input layer, this doesn't necessarily mean that it is the
very first layer in the whole network, but can be the output of a previous layer, 
too).

Let's call the first layer the *hidden* layer. It implements the linear part of 
the *Maxout* units. It is a set of fully-connected layers (the columns in the image) 
with no activation function (which is referred to as *affine*), thus each unit 
in this layer just computes the weighted sum of all inputs, which is defined in 
the second part of the *Maxout* definition above: 

<div>
$$
x^{T} W_{\dots ij} + b_{ij}
$$
</div>

Don't get confused about the biases. In this definition they form a matrix, 
however usually biases are implicit by just adding an additional 1 to the inputs,
so that the weight matrix is slightly bigger than for the regular inputs only.
So actually the bias is an additional weight in the matrix \\(W\\). Maybe think of
the bias matrix in this definition as the slice of weights in the weight matrix.

Now the three-dimensional tensor \\(W\\) contains the weights of this first part. The 
dots in the equation mean that all elements from the first dimension are taken 
like `W[:, i, j]` in Python or `W(:, i, j)` in Matlab. 
Consequently, \\( W_{\dots ij} \\) is the weight vector of the unit in row \\(i\\) and 
column \\(j\\).

In the figure above the units in this first part are aranged in a two-dimensional 
grid. The first dimension of this grid (number of rows) doesn't have to matches 
the number of input units, both \\(j\\) and the second dimension \\(k\\) 
(number of columns) are hyperparameters, which are chosen when 
the whole architecture is defined. These two parameters control the complexity of 
the *Maxout* activation function. The higher the \\(k\\) and \\(j\\) the more accurately
 any convex function can be approximated. Basically each column of units in this 
first part performs linear regression.

The second part is much easier. It is just doing max-pooling over each row of 
the first part, i.e. taking the maximum of the output along each row.

### A simple example

Consider the function \\(f\left(x\right)=x^{2}\\).

We can approximate this function with a single *Maxout* unit that uses three 
linear pieces \\(k=3\\). So it uses three hidden units.

This *Maxout* unit would look like this (biases included this time):

<div id="svg-container-1" class="svg-container"></div>

Each hidden unit calculates:

<div>
$$ z_{j} = x \cdot w_{j} + b_{j} $$
</div>

This is a simple linear function. The max-pooling unit takes the 
maximum of these three linear functions.

Take a look at this picture. It shows the \\(x^{2}\\) function and three linear 
functions that could be learned by the *Maxout* unit.

<img style="display: block; width: 30em; margin: 0.5em auto 0.5em auto" src="./assets/_posts/approximation.svg" alt="Approximation using three linear functions">

Finally, try imagining how this would look like with 4, 5, 6 or an arbitrary 
number of linear functions. That's right, it would be a nice approximation that
is linear everywhere, except for the connection points of the linear parts.

### Where is *Dropout* in all this?

*Dropout* is a regularization mechanism. It simulates the training of a bag of
networks with different architectures. However, this bag of networks contains
only those networks that can be created by dropping an arbitrary number of 
units from the original network. In practice this is implemented by randomly
dropping neurons with a certain probability during training.
As a result in each training pass a kind of different network is trained, however
&ndash; and this is important &ndash; all networks share the same weights, because 
acutally only a single network is used and just the weights of this single 
network are used all the time.

When applying *bagging*, i.e. using \\(n\\) models for prediction rather than just
one, it is necessary to combine the predictions of all models, which can be
easily done by calculating the mean of all predictions. In the *dropout* case
this can not be done, because we actually have only one model. (Ok, it could be
done but it doesn't make sense, since the number of models is way too big). 
Instead, a much better approach is to just scale the weights the whole network
proportional to the drop ratio. The issue then is, that this is only accurate
for a single linear layer with the *Softmax* function applied. Thus, for deeper
models that use a non-linear activation function it is not accurate anymore.

And that is why *Maxout* works as we have seen above. Dropout can be applied to 
the first part of *Maxout* and model averaging is still accurate, because there
are no non-linearities involved. Pretty clever.

