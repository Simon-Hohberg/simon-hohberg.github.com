---
layout: post
title: Convolutional Neural Network's Backpropagation
keywords: CNN, Backpropagation
thumbImg: ./assets/imgs/backprop.png
thumbColor: #F3C74A
---

# Convolutional Neural Network's Backpropagation

Although I knew how to implement an "normal" flat Multilayer Perceptron (MLP),
I got very confused when trying to implement a CNN. In the end I wanted to
fully understand the math to understand how to get a working implementation.
*Isn't this just backpropagation as usual?*
Well, basically yes, but a bit more complex and with some pitfalls, so read on.

### The idea of CNNs, in short

MLPs are usually fully-connected flat networks. A layer is a vector of
neuronal units (thus flat) and each neuron in a layer is connected with every
neuron in the two adjacent layers. As a result the weights between two layers
are represented as a matrix.
Of course such an MLP could be used for images recognition, too. The input
data would be the image reshaped to a vector. So the input layer would need to
have already a lot of units and the whole net would have way to many trainable
parameters (weights) considering fully-connected layers.
Furthermore, such a net would not make use of the inherent property of images
that information is local, rather than distributed over the full image.

Inspired by how the visual cortex works, the idea came up to create nets where
neurons share weights, so not every neuron has its own weight vector, reducing
the number of trainable weights. The weight sharing is implemented in a way so
that the neurons perform a convolution on the data where the filter is formed
by the weights. This also makes more sense for data like
images because two-dimensional features are extracted.
To speed up things even more, usually each convolutional layer is followed by
a pooling layer, whose purpose is just to reduces the size of intermediary
images (feature maps).
In a CNN the image size (feature map size) is reduced by each convolutional
and pooling layer and more and more complex features are extracted. When the
feature maps are small enough they are squashed into a one-dimensional feature
vector which is then fed into a "normal" flat MLP.
This means there are actually two parts in a CNN: The convolutional and pooling
part which extracts two dimensional features that become more complex for
deeper layers. And a conventional MLP part (usually only three layers deep)
that is used when the data is small enough to be processed by a fully-connected
network.


### A simplified view on CNNs

To me it easier to think of CNNs as network of units that input and output
whole images or feature maps. In this view, when dealing with grey-scale
images, there is just a single input unit, which is just the input image
(for RGB images there would be three units, one unit per channel). Following
layers, usually an alternation of convolutional and pooling layers,
then consist of an arbitrary number of units each producing a feature map
(transformed image).

In a convolutional layer each unit receives a number of connections from the
units of the previous layer. For each connection there is a two-dimensional
kernel (or filter) of weights that is used to convolve the image that is
output from a unit of the previous layer to the unit. All the convolved images
are summed by the unit, a bias is added and usually but not always an
activation function is applied one each pixel. The kernels consist of learnable
weights and since each connection has its own kernel, the weights of all
connections to a unit form a three-dimensional tensor.

Units in a pooling layer just take the two-dimensional output of only one unit
in the previous convolutional layer and "pool" it (downscale). Pooling units
can also use a weight and a bias, but usually they do not anymore.


### Convolution & (Cross-) Correlation

Let's first checkout how convolution and correlation are defined.

| | |
|----------:|---------------------------------------|
| \\( i \\) | Input image                           |
| \\( k \\) | Kernel of size \\( N_k \times N_k \\) |
| \\( v \\) | Output image                          |

<h4>Convolution</h4>

<div>
$$
v\left(x,y\right) = \sum_{x_{k}=0}^{N_{k}-1}\sum_{y_{k}=0}^{N_{k}-1}i\left(x-x_{k},y-y_{k}\right)k\left(x_{k},y_{k}\right)
$$
</div>

Important to notice here is that the first pixels in the result are undefined, because \\( i\left(x-x_{k},y-y_{k}\right) \\) is going out of bounds of \\( i\\) for \\( \left( x,y \right) \in \left\\{ \left( 0, 0 \right) \ldots \left( N_k - 1, N_k - 1 \right) \right\\} \\). Thus, looking at the valid pixels only, the resulting image will be \\( N_k - 1\\) pixels smaller than the input image. So although the actual result image begins at \\( \left( N_k - 1, N_k - 1 \right) \\) instead of \\( \left(0,0\right)\\) we won't have the undefined pixels in our result image for convenience, implicitely moving all pixels by \\( \left( N_k - 1, N_k - 1 \right) \\) to the top left.

<h5>Padding</h5>
To overcome the undefined pixels issue, the input image could be padded at the left and top giving an output of the same size as the input. The question then is which values should be used for padding. The most common approaches are padding with zeros or wrapping the image around.

<h4>(Cross-) Correlation</h4>

<div>
$$
l\left(x,y\right) = \sum_{x_{k}=0}^{N_{k}-1}\sum_{y_{k}=0}^{N_{k}-1}i\left(x+x_{k},y+y_{k}\right)k\left(x_{k},y_{k}\right)
$$
</div>

To me correlation is more intuitive than covnolution &ndash; everything is pretty straight forward.

Here we also have the issue with undefined pixels but instead of the top left it is the bottom right. The same padding mechanisms as with convolution can be applied.

### Let's talk Math

I am considering the feedforward and backpropagation for convolutional layers
only, since pooling layers (without weights) are boring. For completeness, when
implementing backpropagation one just upscales the backpropagated error in the
same way as the input was downscaled.

### Naming

| | |
|----------------------------------------:|--------------------------------------------------------------------------------------------------------------------------------|
|  \\( i_{m,n}^{l}\left(x,\, y\right) \\) | Input from unit \\( m \\) at layer \\( l-1 \\) to unit \\( n \\) at layer \\( l \\)                              |
|  \\( w_{m,n}^{l}\left(x,\, y\right) \\) | kernel (weights) for connection between unit \\( m \\) at layer \\( l-1 \\) to unit \\( n \\) at layer \\( l \\) |
|    \\( c_{n}^{l}\left(x,\, y\right) \\) | Complete input to unit \\( n \\) at layer \\( l \\)                                                                     |
|                       \\( b_{n}^{l} \\) | Bias of unit \\( n \\) at layer \\( l \\)                                                                               |
|                            \\( a(x) \\) | Activation function                                                                                                            |
|    \\( o_{n}^{l}\left(x,\, y\right) \\) | Output of unit \\( n \\) at layer \\( l \\)                                                                             |
|                               \\( t \\) | Target (Label)                                                                                                                 |


<!--<div id="svg-container" class="svg-container"><svg></svg></div>-->

<!--<script type="text/javascript">-->
<!--  mathJaxRendered = function() {-->
<!--    var svg = $("#svg-container", "svg");-->
<!--    addMathToSvg("mathJaxSource", svg);-->
<!--  }-->
<!--  -->
<!--  MathJax.Hub.Register.StartupHook("End Typeset", mathJaxRendered);-->
<!--  -->
<!--  var svgWidth = 800;-->
<!--  var svgHeight = 400;-->
<!--  var raphael = Raphael("svg-container", '100%', '100%');-->
<!--  raphael.setViewBox(0, 0, svgWidth, svgHeight, true);-->
<!--  $("#svg-container").css("padding-bottom", ((svgHeight/svgWidth)*100) + "%")-->
<!--  -->
<!--  var grid = new Grid(10, 10, 8, 8, 10, 10);-->
<!--  -->
<!--  var net = new Net("     |---(sum|bias|activ)---|                \\"-->
<!--                  + "G1 --o---(sum|bias|activ)----(sum|bias|activ)\\"-->
<!--                  + "     |          .           |                \\"-->
<!--                  + "     |          .           |                \\"-->
<!--                  + "     |          .           |                \\"-->
<!--                  + "     |---(sum|bias|activ)---|                  ",-->
<!--                  "{G1: [10, 10, 8, 8, 10, 10]}");-->
<!--  var neuronWidth = 80;-->
<!--  var neuronHeight = 20;-->
<!--  -->
<!--  net.addNode(new PillNode(300, 50, neuronWidth, neuronHeight));-->
<!--  net.addNode(new PillNode(300, 150, neuronWidth, neuronHeight));-->
<!--  net.addNode(new PillNode(300, 300, neuronWidth, neuronHeight));-->
<!--  net.addNode(new PillNode(500, 150, neuronWidth, neuronHeight));-->
<!--  net.link(0, 3).link(1, 3).link(2, 3);-->
<!--  -->
<!--  // draw-->
<!--  grid.draw(raphael);-->
<!--  net.draw(raphael);-->
<!--</script>-->
<!--<div id="mathJaxSource" style="display: none">-->
<!--  <math xmlns="http://www.w3.org/1998/Math/MathML"><mo>&#8721;</mo><mi>c</mi><mo>+</mo><mi>&#946;</mi></math>-->
<!--</div>-->


### Feed-Forward

#### Single Input

<div>
$$ i_{n,m}^l\left(x,\, y \right) = \sum_{x',\, y'} o_{n}^{l-1}\left(x-x',\, y-y'\right) \cdot w_{n,m}^{l}\left(x',\, y' \right) $$
</div>

One input to a convolutional unit is the convolution of the ouput from a unit
in the previous layer. The kernel of this convolution is used just for this
connection.

#### Complete Input

<div>
$$ \begin{aligned} c_{m}^l\left(x,\, y\right) \, & = \sum_{n} i_{n,m}^{l}\left(x,\,y\right) + b_{m}^{l} \\
  &= \sum_{n,\, x',\, y'} o_{n}^{l-1}\left(x-x',\, y-y'\right) \cdot w_{n,m}^{l}\left(x',\, y' \right) + b_{m}^{l} \end{aligned} $$
</div>

Usually a unit has inputs from multiple units in the previous layer. All the
single inputs to the unit are summed up and the bias is added to form the
complete input to it.

#### Output

<div>
$$ o_{m}^{l}\left(x,\, y\right) = a\left(c_{m}^{l}\left(x,\, y\right)\right)$$
</div>

The output of the unit is obtained by applying the activation function on each
pixel.

#### Error

<div>
$$ E = \frac{1}{2} \left( t - o^L \right)^{2} $$
</div>

Let's assume we use the mean squared error to measure how well our CNN performs
on a given input with respect to the given label \\( t\\), where \\( o^L\\) is
the output of the last layer in the CNN.

### Backpropagation

#### Weight Update

<div>
$$
\frac{\partial E}{\partial w_{n,m}^{l}\left(x,\, y\right)} = \sum_{x',y'}\underbrace{\frac{\partial E}{\partial c_{m}^{l}\left(x',\, y'\right)}}_{\delta_{m}^{l}\left(x',\, y' \right)} \cdot \frac{\partial c_{m}^{l}\left(x',\, y'\right)}{\partial w_{n,m}^{l}\left(x,\, y\right)}
$$
</div>

<div>
$$
\begin{aligned}
\frac{\partial c_{m}^{l}\left(x',\, y'\right)}{\partial w_{n,m}^{l}\left(x,\, y\right)} &= \frac{\partial}{\partial w_{n,m}^{l}\left(x,\, y\right)} \left( \sum_{n,\, x'',\, y''} o_{n}^{l-1}\left(x'-x'',\, y'-y''\right) \cdot w_{n,m}^{l}\left(x'',\, y'' \right) + b_{m}^{l} \right) \\
  &= \frac{\partial}{\partial w_{n,m}^{l}\left(x,\, y\right)} \left( o_{0}^{l-1}\left( x'-0,\, y'-0 \right) \cdot w_{0,m}^{l}\left( 0, \, 0 \right) + \ldots + o_{n}^{l-1}\left( x' - x,\, y' - y \right) \cdot w_{n,m}^{l} \left( x,\, y \right) + \ldots + b_{m}^{l} \right) \\
  &= o_{n}^{l-1}\left( x'-x,\, y' - y \right)
\end{aligned}
$$
</div>

<div>
$$
\frac{\partial E}{\partial w_{n,m}^{l}\left(x,\, y\right)} = \sum_{x',y'} o_{n}^{l-1}\left( x'-x,\, y' - y \right) \cdot \delta_{m}^{l}\left(x',\, y' \right)
$$
</div>

So far so good, but what is this result really? Especially what does this sum mean?

We would like to know how we have to change the weight \\( w_{n,m}^{l}\left(x,y\right) \\) to reduce the error and are given the \\( \delta_{m}^{l} \\) which is how the output of the convolution of \\( o_{n}^{l-1}\\) and \\( w_{n,m}^{l}\\) has to change. The update for the weight is now the change of the output of the convolution \\( \delta_{m}^{l} \\) with regard to the input of the convolution \\( o_{n}^{l-1}\\), considering all pixels in \\( o_{n}^{l-1}\\) where the weight \\( w_{n,m}^{l}\left(x,y\right) \\) is involved.

How can this be implemented? It looks like a convolution at first glance where \\( \delta_{m}^{l} \\) is the kernel, since we are iterating over it. However it has got more in common with a correlation, because the kernel is applied in the correct orientation instead of being applied upside down as in a convolution. But there is a negative instead of a positive offset which makes it different.

We have to go back to the definition of the convolution to get to know how to implement this. We said there that the first valid pixels of the result image do not start at \\( \left(0,0\right) \\) but with an offset. More importantly we also said that we will implicitely move the valid result pixels by \\( \left(N_k-1,N_k-1\right) \\) where \\( N_k\\) was the kernel size so that the result image starts at \\( \left(0,0\right)\\) again.

How is this related to this equation? Well, the \\( \delta_{m}^{l} \\) contains the partial derivatives for the complete input  \\( c_{m}^{l}\\)) of our unit. This input \\( c_{m}^{l}\\) is the result of a convolution and thus when referencing a pixel in \\( c_{m}^{l}\\) with regard to \\(o_{n}^{l-1}\\) &ndash; which was the input to the convolution &ndash; we have to take into account the implicit offset created by the convolution.

Looking only at the \\( o_{n}^{l-1}\\) in the sum, making the implicit offset explicit again, we get: \\( o_{n}^{l-1}\left( x' + N_w-1 - x,\, y' + N_w-1 - y \right) \\), where \\( N_w\\) is the size of the weight kernel. \\( \left(N_w-1-x, N_w-1-y\right) \\) basically references the weight kernel upside down. So when referencing \\( o_{n}^{l-1}\\) like \\( o_{n}^{l-1}\left( x'+x,\, y'+y\right) \\) the result is the same but upside down &ndash; nice! It's a correlation!

<div>
$$
\frac{\partial E}{\partial w_{n,m}^{l}\left(x,\, y\right)} = \operatorname{rot}_{180}\left(\underbrace{\sum_{x',y'} o_{n}^{l-1}\left( x + x',\, y + y' \right) \cdot \delta_{m}^{l}\left(x',\, y' \right)}_{\text{Cross-Correlation}}\right)
$$
</div>

#### Deltas

<div>
$$
\delta_{m}^{l}\left(x,\: y\right)=\frac{\partial E}{\partial c_{m}^{l}\left(x,\, y\right)}=\sum_{o}\sum_{\, x',\, y'}^{N_{w},N_{w}}\underbrace{\frac{\partial E}{\partial c_{o}^{l+1}\left(x+x',\, y+y'\right)}}_{\delta_{o}^{l+1}\left(x+x',\, y+y'\right)}\cdot\frac{\partial c_{o}^{l+1}\left(x+x',\, y+y'\right)}{\partial c_{m}^{l}\left(x,\, y\right)}
$$
</div>

<div>
$$
\begin{aligned}

\frac{\partial c_{o}^{l+1}\left(x+x',\, y+y'\right)}{\partial c_{m}^{l}\left(x,\, y\right)} & =\frac{\partial}{\partial c_{m}^{l}\left(x,\, y\right)}\left(\sum_{m'}\sum_{\, x'',\, y''}^{N_{w},N_{w}}o_{m'}^{l}\left(x+x'-x'',\, y+y'-y''\right)\cdot w_{m',o}^{l+1}\left(x'',\, y''\right)\right)\\
 & =\frac{\partial}{\partial c_{m}^{l}\left(x,\, y\right)}\left(\sum_{m'}\sum_{\, x'',\, y''}^{N_{w},N_{w}}a\left(c_{m'}^{l}\left(x+x'-x'',\, y+y'-y''\right)\right)\cdot w_{m',o}^{l+1}\left(x'',\, y''\right)\right)\\
 & =\frac{\partial}{\partial c_{m}^{l}\left(x,\, y\right)}\left(w_{0,o}^{l+1}\left(0,\,0\right)\cdot a\left(c_{0}^{l}\left(x'-0,\, y'-0\right)\right)+\ldots+w_{m,o}^{l+1}\left(x'',\, y''\right)\cdot a\left(c_{m}^{l}\left(x,\, y\right)\right)+\ldots\right)\\
 & =w_{m,o}^{l+1}\left(x+x',\, y+y'\right)\cdot\frac{\partial a\left(c_{m}^{l}\left(x,\, y\right)\right)}{\partial c_{m}^{l}\left(x,\, y\right)}

\end{aligned}
$$
</div>

<div>
$$
\delta_{m}^{l}\left(x,\: y\right)=\underbrace{\sum_{o}\sum_{\, x',\, y'}^{N_{w},N_{w}}\delta_{o}^{l+1}\left(x+x',\, y+y'\right)\cdot w_{m,o}^{l+1}\left(x',\, y'\right)}_{\text{Cross-Correlation, Backpropagated Error}}\cdot\frac{\partial a\left(c_{m}^{l}\left(x,\, y\right)\right)}{\partial c_{m}^{l}\left(x,\, y\right)}
$$
</div>
