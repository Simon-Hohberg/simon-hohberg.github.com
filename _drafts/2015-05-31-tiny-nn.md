---
layout: post
title: Implementing a Neural Network in Python
---

Abstract

### Naming

| | |
|--------------------------------:|-------------------------------------------------------------------------------------------------------|
|  \\( i_{m,n}^{l} \\)            | Input from unit \\( m \\) at layer \\( l-1 \\) to unit \\( n \\) at layer \\( l \\)                   |
|  \\( w_{m,n}^{l} \\)            | Weight of connection between unit \\( m \\) at layer \\( l-1 \\) to unit \\( n \\) at layer \\( l \\) |
|    \\( c_{n}^{l} \\)            | Complete input to unit \\( n \\) at layer \\( l \\)                                                   |
| \\( \operatorname{a}(\cdot) \\) | Activation function                                                                                   |
|   \\( o_{n}^{l} \\)             | Output of unit \\( n \\) at layer \\( l \\)                                                           |
|        \\( t \\)                | Target (Label)                                                                                        |

<script type="text/javascript" src="/js/raphael/raphael-min.js"></script>
<script type="text/javascript" src="/js/raphael/raphael-utils.js"></script>
<script type="text/javascript" src="/js/raphael/text-along-path.js"></script>
<script type="text/javascript" src="/js/raphael/net/nodes.js"></script>
<script type="text/javascript" src="/js/raphael/net/links.js"></script>
<script type="text/javascript" src="/js/raphael/net/net.js"></script>
<link rel="stylesheet" href="/css/svg.css">

<div id="svg-container" class="svg-container"><svg></svg></div>

<script type="text/javascript">
  mathJaxRendered = function() {
    var svg = $("#svg-container", "svg");
    addMathToSvg("mathJaxSource", svg);
  }
  
  MathJax.Hub.Register.StartupHook("End Typeset", mathJaxRendered);
  
  var svgWidth = 400;
  var svgHeight = 200;
  var raphael = Raphael("svg-container", '50%', '50%');
  raphael.setViewBox(0, 0, svgWidth, svgHeight, true);
  $("#svg-container").css("padding-bottom", ((svgHeight/svgWidth)*100) + "%")
  
  var net = new Net();
  var neuronWidth = 80;
  var neuronHeight = 20;
  
  net.addNode(new PillNode(50, 50, neuronWidth, neuronHeight));
  net.addNode(new PillNode(250, 100, neuronWidth, neuronHeight));
  net.link(0, 1);
  
  // draw
  net.draw(raphael);
</script>


### Feed-Forward

#### Single Input

$$ i_{n,m}^l = o_{n}^{l-1} \cdot w_{n,m}^{l} $$

#### Complete Input

$$ c_{m}^l = \sum_{n} i_{n,m}^{l} + \underbrace{w_{n+1,m}^{l}}_{\text{Bias}} $$

#### Output

$$ o_{m}^{l} = a\left(c_{m}^{l}\right)$$

#### Error

$$ E = \frac{1}{2} \left( t - o^L \right)^{2} $$


### Backpropagation

#### Weight Update

$$
\frac{\partial E}{\partial w_{n,m}^{l}} = \underbrace{\frac{\partial E}{\partial c_{m}^{l}}}_{\delta_{m}^{l}} \cdot \frac{\partial c_{m}^{l}}{\partial w_{n,m}^{l}}
$$

$$
\begin{aligned}
\frac{\partial c_{m}^{l}}{\partial w_{n,m}^{l}} &= \frac{\partial}{\partial w_{n,m}^{l}} \left( \sum_{n} i_{n,m}^{l} + w_{n+1,m}^{l} \right) \\
  &= \frac{\partial}{\partial w_{n,m}^{l}} \left( \sum_{n} o_{n}^{l-1} \cdot w_{n,m}^{l} + w_{n+1,m}^{l} \right) \\
  &= o_{n}^{l-1}
\end{aligned}
$$

$$
\frac{\partial E}{\partial w_{n,m}^{l}} = \delta_{m}^{l} \cdot o_{n}^{l-1}
$$

#### Deltas

$$
\delta_{m}^{l} = \frac{\partial E}{\partial c_{m}^{l}}=\sum_{o}\underbrace{\frac{\partial E}{\partial c_{o}^{l+1}}}_{\delta_{o}^{l+1}}\cdot\frac{\partial c_{o}^{l+1}}{\partial c_{m}^{l}}
$$

$$
\begin{aligned}
\frac{\partial c_{o}^{l+1}}{\partial c_{m}^{l}} & =\frac{\partial}{\partial c_{m}^{l}}\left(\sum_{m'} o_{m'}^{l} \cdot w_{m',o}^{l+1} \right) = \frac{\partial}{\partial c_{m}^{l}}\left(\sum_{m'} \operatorname{a}\left(c_{m'}^{l}\right) \cdot w_{m',o}^{l+1} \right) = \frac{\partial \operatorname{a}\left(c_{m}^{l}\right)}{\partial c_{m}^{l}} \cdot w_{m,o}^{l+1} \\
\end{aligned}
$$

$$
\delta_{m}^{l} = \underbrace{\sum_{o} \delta_{o}^{l+1} \cdot w_{m,o}^{l+1}}_{\text{Backpropagated Error}} \cdot \frac{\partial \operatorname{a}\left(c_{m}^{l}\right)}{\partial c_{m}^{l}}
$$

