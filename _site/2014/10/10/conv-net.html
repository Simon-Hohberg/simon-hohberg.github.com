<meta charset="utf-8">

<script type="text/javascript" src="/js/jquery/jquery-2.1.1.min.js"></script>
<script type="text/javascript" src="/js/jquery/jquery-ui.min.js"></script>
<script type="text/x-mathjax-config">
if( /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent) ) {
  MathJax.Hub.Config({
  "HTML-CSS": { linebreaks: { automatic: true } },
         SVG: { linebreaks: { automatic: true }, scale: 90 }
  });
} else {
  MathJax.Hub.Config({
    "HTML-CSS": { linebreaks: { automatic: true } },
           SVG: { linebreaks: { automatic: true } }
  });
}
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG"></script>
<link rel="stylesheet" href="/css/skeleton.css">
<link rel="stylesheet" href="/css/base.css">
<link rel="stylesheet" href="/css/layout.css">
<link rel="stylesheet" href="/css/table.css">
<link rel="stylesheet" href="/css/jquery-ui/jquery-ui.min.css">
<link rel="stylesheet" href="/css/jquery-ui/jquery-ui.structure.min.css">
<link rel="stylesheet" href="/css/jquery-ui/jquery-ui.theme.css">

<article class="container">
  <header><h2>Convolutional Neural Networks</h2></header>
  <div><p>A formal approach to convolutional neural networks.</p>

<script type="text/javascript" src="/js/raphael/raphael-min.js"></script>

<script type="text/javascript" src="/js/raphael/raphael-utils.js"></script>

<link rel="stylesheet" href="/css/svg.css" />

<script type="text/javascript">
  $(function() {
    $( ".explain-accordion" ).accordion({
      collapsible: true,
      active: false,
      heightStyle: "content"
    });
  });
</script>

<h3 id="convolution--cross--correlation">Convolution &amp; (Cross-) Correlation</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: right"> </th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">\( i \)</td>
      <td>Input image</td>
    </tr>
    <tr>
      <td style="text-align: right">\( k \)</td>
      <td>Kernel of size \( N_k \times N_k \)</td>
    </tr>
    <tr>
      <td style="text-align: right">\( v \)</td>
      <td>Output image</td>
    </tr>
  </tbody>
</table>

<h4 id="convolution">Convolution</h4>

<div class="math-definition">
$$
v\left(x,y\right) = \sum_{x_{k}=0}^{N_{k}-1}\sum_{y_{k}=0}^{N_{k}-1}i\left(x-x_{k},y-y_{k}\right)k\left(x_{k},y_{k}\right)
$$
</div>

<div class="explain-accordion">
  <h4>Explain</h4>
  <div>
    <p>
    Important to notice here is that the first pixels in the result are undefined, because \( i\left(x-x_{k},y-y_{k}\right) \) is going out of bounds of \( i \) for \( \left( x,y \right) \in \left\{ \left( 0, 0 \right) \ldots \left( N_k - 1, N_k - 1 \right) \right\}  \). Thus, looking at the valid pixels only, the resulting image will be \( N_k - 1 \) pixels smaller than the input image. So although the actual result image begins at \( \left( N_k - 1, N_k - 1 \right) \) instead of \( \left(0,0\right)\) we won't have the undefined pixles in our result image for convenience, implicitely moving all pixels by \( \left( N_k - 1, N_k - 1 \right) \) to the top left.

<h5>Padding</h5>
To overcome the undefined pixels issue, the input image could be padded at the left and top giving an output of the same size as the input. The question then is which values should be used for padding. The most common approaches are padding with zeros or wrapping the image around.
    </p>
  </div>
</div>

<h4 id="cross--correlation">(Cross-) Correlation</h4>

<div class="math-definition">
$$
l\left(x,y\right) = \sum_{x_{k}=0}^{N_{k}-1}\sum_{y_{k}=0}^{N_{k}-1}i\left(x+x_{k},y+y_{k}\right)k\left(x_{k},y_{k}\right)
$$
</div>
<div class="explain-accordion">
  <h4>Explain</h4>
  <div>
    <p>
To me correlation is more intuitive than covnolution &ndash; everything is pretty straight forward.

Here we also have the issue with undefined pixels but instead of the top left it is the bottom right. The same padding mechanisms as with convolution can be applied.
    </p>
  </div>
</div>

<h3 id="naming">Naming</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: right"> </th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">\( i_{m,n}^{l}\left(x,\, y\right) \)</td>
      <td>Input from feature map \( m \) at layer \( l-1 \) to feature map \( n \) at layer \( l \)</td>
    </tr>
    <tr>
      <td style="text-align: right">\( w_{m,n}^{l}\left(x,\, y\right) \)</td>
      <td>kernel (weights) for connection between feature map \( m \) at layer \( l-1 \) to feature map \( n \) at layer \( l \)</td>
    </tr>
    <tr>
      <td style="text-align: right">\( c_{n}^{l}\left(x,\, y\right) \)</td>
      <td>Complete input to feature map \( n \) at layer \( l \)</td>
    </tr>
    <tr>
      <td style="text-align: right">\( b_{n}^{l} \)</td>
      <td>Bias of feature map \( n \) at layer \( l \)</td>
    </tr>
    <tr>
      <td style="text-align: right">\( a(x) \)</td>
      <td>Activation function</td>
    </tr>
    <tr>
      <td style="text-align: right">\( o_{n}^{l}\left(x,\, y\right) \)</td>
      <td>Output of feature map \( n \) at layer \( l \)</td>
    </tr>
    <tr>
      <td style="text-align: right">\( t \)</td>
      <td>Target (Label)</td>
    </tr>
  </tbody>
</table>

<h3 id="feed-forward">Feed-Forward</h3>

<h4 id="single-input">Single Input</h4>
<div class="math-definition">
$$ i_{n,m}^l\left(x,\, y \right) = \sum_{x',\, y'} o_{n}^{l-1}\left(x-x',\, y-y'\right) \cdot w_{n,m}^{l}\left(x',\, y' \right) $$
</div>

<h4 id="complete-input">Complete Input</h4>
<div class="math-definition">
$$ \begin{aligned} c_{m}^l\left(x,\, y\right) \, &amp; = \sum_{n} i_{n,m}^{l}\left(x,\,y\right) + b_{m}^{l} \\
  &amp;= \sum_{n,\, x',\, y'} o_{n}^{l-1}\left(x-x',\, y-y'\right) \cdot w_{n,m}^{l}\left(x',\, y' \right) + b_{m}^{l} \end{aligned} $$
</div>

<h4 id="output">Output</h4>
<div class="math-definition">
$$ o_{m}^{l}\left(x,\, y\right) = a\left(c_{m}^{l}\left(x,\, y\right)\right)$$
</div>

<h4 id="error">Error</h4>
<div class="math-definition">
$$ E = \frac{1}{2} \left( t - o^L \right)^{2} $$
</div>

<h3 id="backpropagation">Backpropagation</h3>

<h4 id="weight-update">Weight Update</h4>
<div class="math-definition">
$$
\frac{\partial E}{\partial w_{n,m}^{l}\left(x,\, y\right)} = \sum_{x',y'}\underbrace{\frac{\partial E}{\partial c_{m}^{l}\left(x',\, y'\right)}}_{\delta_{m}^{l}\left(x',\, y' \right)} \cdot \frac{\partial c_{m}^{l}\left(x',\, y'\right)}{\partial w_{n,m}^{l}\left(x,\, y\right)}
$$
</div>

<div class="math-definition">
$$
\begin{aligned}
\frac{\partial c_{m}^{l}\left(x',\, y'\right)}{\partial w_{n,m}^{l}\left(x,\, y\right)} &amp;= \frac{\partial}{\partial w_{n,m}^{l}\left(x,\, y\right)} \left( \sum_{n,\, x'',\, y''} o_{n}^{l-1}\left(x'-x'',\, y'-y''\right) \cdot w_{n,m}^{l}\left(x'',\, y'' \right) + b_{m}^{l} \right) \\
  &amp;= \frac{\partial}{\partial w_{n,m}^{l}\left(x,\, y\right)} \left( o_{0}^{l-1}\left( x'-0,\, y'-0 \right) \cdot w_{0,m}^{l}\left( 0, \, 0 \right) + \ldots + o_{n}^{l-1}\left( x' - x,\, y' - y \right) \cdot w_{n,m}^{l} \left( x,\, y \right) + \ldots + b_{m}^{l} \right) \\
  &amp;= o_{n}^{l-1}\left( x'-x,\, y' - y \right)
\end{aligned}
$$
</div>

<div class="math-definition">
$$
\frac{\partial E}{\partial w_{n,m}^{l}\left(x,\, y\right)} = \sum_{x',y'} o_{n}^{l-1}\left( x'-x,\, y' - y \right) \cdot \delta_{m}^{l}\left(x',\, y' \right)
$$
</div>

<div class="explain-accordion">
  <h4>Explain</h4>
  <div>
    <p>
      So far so good, but what is this result really? Especially what does this sum mean?
    </p>
    <p>
      We would like to know how we have to change the weight \( w_{n,m}^{l}\left(x,y\right) \) to reduce the error and are given the \( \delta_{m}^{l} \) which is how the output of the convolution of \( o_{n}^{l-1} \) and \( w_{n,m}^{l} \) has to change. The update for the weight is now the change of the output of the convolution \( \delta_{m}^{l} \) with regard to the input of the convolution \( o_{n}^{l-1} \), considering all pixels in \( o_{n}^{l-1} \) where the weight \( w_{n,m}^{l}\left(x,y\right) \) is involved.
    </p>

    <p>
      How can this be implemented? It looks like a convolution at first glance where \( \delta_{m}^{l} \) is the kernel, since we are iterating over it. However it has got more in common with a correlation, because the kernel is applied in the correct orientation instead of being applied upside down as in a convolution. But there is a negative instead of a positive offset which makes it different.
    </p>

    <p>
      We have to go back to the definition of the convolution to get to know how to implement this. We said there that the first valid pixels of the result image do not start at \( \left(0,0\right) \) but with an offset. More importantly we also said that we will implicitely move the valid result pixels by \( \left(N_k-1,N_k-1\right) \) where \( N_k \) was the kernel size so that the result image starts at \( \left(0,0\right)\) again.
    </p>

    <p>
      How is this related to this equation? Well, the \( \delta_{m}^{l} \) contains the partial derivatives for the complete input (\( c_{m}^{l}\)) of our neuron. This input \( c_{m}^{l}\) is the result of a convolution and thus when referencing a pixel in \( c_{m}^{l}\) with regard to \(o_{n}^{l-1}\) &ndash; which was the input to the convolution &ndash; we have to take into account the implicit offset created by the convolution.
    </p>

    <p>
      Looking only at the \( o_{n}^{l-1} \) in the sum, making the implicit offset explicit again, we get: \( o_{n}^{l-1}\left( x' + N_w-1 - x,\, y' + N_w-1 - y \right) \), where \( N_w \) is the size of the weight kernel. \( \left(N_w-1-x, N_w-1-y\right) \) basically references the weight kernel upside down. So when referencing \( o_{n}^{l-1} \) like \( o_{n}^{l-1}\left( x'+x,\, y'+y\right) \) the result is the same but upside down &ndash; nice! It's a correlation!
    </p>
  </div>
</div>

<div class="math-definition">
$$
\frac{\partial E}{\partial w_{n,m}^{l}\left(x,\, y\right)} = \operatorname{rot}_{180}\left(\underbrace{\sum_{x',y'} o_{n}^{l-1}\left( x + x',\, y + y' \right) \cdot \delta_{m}^{l}\left(x',\, y' \right)}_{\text{Cross-Correlation}}\right)
$$
</div>

<h4 id="deltas">Deltas</h4>
<div class="math-definition">
$$
\delta_{m}^{l}\left(x,\: y\right)=\frac{\partial E}{\partial c_{m}^{l}\left(x,\, y\right)}=\sum_{o}\sum_{\, x',\, y'}^{N_{w},N_{w}}\underbrace{\frac{\partial E}{\partial c_{o}^{l+1}\left(x+x',\, y+y'\right)}}_{\delta_{o}^{l+1}\left(x+x',\, y+y'\right)}\cdot\frac{\partial c_{o}^{l+1}\left(x+x',\, y+y'\right)}{c_{m}^{l}\left(x,\, y\right)}
$$
</div>

<div class="math-definition">
$$
\begin{aligned}

\frac{\partial c_{o}^{l+1}\left(x+x',\, y+y'\right)}{\partial c_{m}^{l}\left(x,\, y\right)} &amp; =\frac{\partial}{\partial c_{m}^{l}\left(x,\, y\right)}\left(\sum_{m'}\sum_{\, x'',\, y''}^{N_{w},N_{w}}o_{m'}^{l}\left(x+x'-x'',\, y+y'-y''\right)\cdot w_{m',o}^{l+1}\left(x'',\, y''\right)\right)\\
 &amp; =\frac{\partial}{\partial c_{m}^{l}\left(x,\, y\right)}\left(\sum_{m'}\sum_{\, x'',\, y''}^{N_{w},N_{w}}a\left(c_{m'}^{l}\left(x+x'-x'',\, y+y'-y''\right)\right)\cdot w_{m',o}^{l+1}\left(x'',\, y''\right)\right)\\
 &amp; =\frac{\partial}{\partial c_{m}^{l}\left(x,\, y\right)}\left(w_{0,o}^{l+1}\left(0,\,0\right)\cdot a\left(c_{0}^{l}\left(x'-0,\, y'-0\right)\right)+\ldots+w_{m,o}^{l+1}\left(x'',\, y''\right)\cdot a\left(c_{m}^{l}\left(x,\, y\right)\right)+\ldots\right)\\
 &amp; =w_{m,o}^{l+1}\left(x+x',\, y+y'\right)\cdot\frac{\partial a\left(c_{m}^{l}\left(x,\, y\right)\right)}{\partial c_{m}^{l}\left(x,\, y\right)}

\end{aligned}
$$
</div>

<div class="math-definition">
$$
\delta_{m}^{l}\left(x,\: y\right)=\underbrace{\sum_{o}\sum_{\, x',\, y'}^{N_{w},N_{w}}\delta_{o}^{l+1}\left(x+x',\, y+y'\right)\cdot w_{m,o}^{l+1}\left(x',\, y'\right)}_{\text{Cross-Correlation, Backpropagated Error}}\cdot\frac{\partial a\left(c_{m}^{l}\left(x,\, y\right)\right)}{\partial c_{m}^{l}\left(x,\, y\right)}
$$
</div>

<!--<div id="svg-container" class="svg-container"></div>-->

<!--<script type="text/javascript">-->
<!--  mathJaxRendered = function() {-->
<!--    var svg = $("svg");-->
<!--    addMathToSvg("mathJaxSource", svg);-->
<!--  }-->
<!--  -->
<!--  MathJax.Hub.Register.StartupHook("End Typeset", mathJaxRendered);-->
<!--  -->
<!--  var raphael = Raphael("svg-container", '100%', '100%');-->
<!--  raphael.setViewBox(0, 0, 800, 800, true);-->
<!--  -->
<!--  var grid = new Grid(10, 10, 8, 8, 10, 10);-->
<!--  -->
<!--  var neurons = [];-->
<!--  var links = [];-->
<!--  neurons.push(new Node(300, 50, 80, 20));-->
<!--  neurons.push(new Node(300, 150, 80, 20));-->
<!--  neurons.push(new Node(500, 50, 80, 20));-->
<!--  neurons.push(new Node(500, 150, 80, 20));-->
<!--  -->
<!--  links.push(neurons[0].linkTo(neurons[2]));-->
<!--  links.push(neurons[0].linkTo(neurons[3]));-->
<!--  links.push(neurons[1].linkTo(neurons[2]));-->
<!--  links.push(neurons[1].linkTo(neurons[3]));-->
<!--  -->
<!--  // draw-->
<!--  grid.draw(raphael);-->
<!--  for (var i = 0; i < neurons.length; i++) {-->
<!--    neurons[i].draw(raphael);-->
<!--  }-->
<!--  for (var i = 0; i < links.length; i++) {-->
<!--    links[i].draw(raphael);-->
<!--  }-->
<!--</script>-->
<!--<div id="mathJaxSource" style="display: none">-->
<!--  <math xmlns="http://www.w3.org/1998/Math/MathML"><mo>&#8721;</mo><mi>c</mi><mo>+</mo><mi>&#946;</mi></math>-->
<!--</div>-->

</div>
</article>
