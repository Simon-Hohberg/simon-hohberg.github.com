<meta charset="utf-8">

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  "HTML-CSS": { linebreaks: { automatic: true } },
         SVG: { linebreaks: { automatic: true } }
});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG"></script>
<link rel="stylesheet" href="/css/skeleton.css">
<link rel="stylesheet" href="/css/base.css">
<link rel="stylesheet" href="/css/layout.css">
<link rel="stylesheet" href="/css/table.css">

<article class="container">
  <header><h2>Convolutional Neural Networks</h2></header>
  <div><p>A formal approach to convolutional neural networks.</p>

<script type="text/javascript" src="/js/raphael-min.js"></script>

<script type="text/javascript" src="/js/jquery-2.1.1.min.js"></script>

<script type="text/javascript" src="/js/raphael-utils.js"></script>

<link rel="stylesheet" href="/css/svg.css" />

<h3 id="convolution--cross--correlation">Convolution &amp; (Cross-) Correlation</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: right"> </th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">\( i \)</td>
      <td>Image</td>
    </tr>
    <tr>
      <td style="text-align: right">\( k \)</td>
      <td>Kernel of size \( N_k \times N_k \)</td>
    </tr>
  </tbody>
</table>

<h4 id="convolution">Convolution</h4>
<div class="math-definition">
$$
v\left(x,y\right) = \sum_{x_{k}=0}^{N_{k}-1}\sum_{y_{k}=0}^{N_{k}-1}i\left(x-x_{k},y-y_{k}\right)k\left(x_{k},y_{k}\right)
$$
</div>

<h4 id="cross--correlation">(Cross-) Correlation</h4>
<div class="math-definition">
$$
l\left(x,y\right) = \sum_{x_{k}=0}^{N_{k}-1}\sum_{y_{k}=0}^{N_{k}-1}i\left(x+x_{k},y+y_{k}\right)k\left(x_{k},y_{k}\right)
$$
</div>

<h3 id="naming">Naming</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: right"> </th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">\( i_{m,n}^{l}\left(x,\, y\right) \)</td>
      <td>Input from feature map \( m \) at layer \( l-1 \) to feature map \( n \) at layer \( l \)</td>
    </tr>
    <tr>
      <td style="text-align: right">\( w_{m,n}^{l}\left(x,\, y\right) \)</td>
      <td>kernel (weights) for connection between feature map \( m \) at layer \( l-1 \) to feature map \( n \) at layer \( l \)</td>
    </tr>
    <tr>
      <td style="text-align: right">\( c_{n}^{l}\left(x,\, y\right) \)</td>
      <td>Complete input to feature map \( n \) at layer \( l \)</td>
    </tr>
    <tr>
      <td style="text-align: right">\( b_{n}^{l} \)</td>
      <td>Bias of feature map \( n \) at layer \( l \)</td>
    </tr>
    <tr>
      <td style="text-align: right">\( a(x) \)</td>
      <td>Activation function</td>
    </tr>
    <tr>
      <td style="text-align: right">\( o_{n}^{l}\left(x,\, y\right) \)</td>
      <td>Output of feature map \( n \) at layer \( l \)</td>
    </tr>
    <tr>
      <td style="text-align: right">\( t \)</td>
      <td>Target (Label)</td>
    </tr>
  </tbody>
</table>

<h3 id="feed-forward">Feed-Forward</h3>

<h4 id="single-input">Single Input</h4>
<div class="math-definition">
$$ i_{n,m}^l\left(x,\, y \right) = \sum_{x',\, y'} o_{n}^{l-1}\left(x-x',\, y-y'\right) \cdot w_{n,m}^{l}\left(x',\, y' \right) $$
</div>

<h4 id="complete-input">Complete Input</h4>
<div class="math-definition">
$$ \begin{aligned} c_{m}^l\left(x,\, y\right) \, &amp; = \sum_{n} i_{n,m}^{l}\left(x,\,y\right) + b_{m}^{l} \\
  &amp;= \sum_{n,\, x',\, y'} o_{n}^{l-1}\left(x-x',\, y-y'\right) \cdot w_{n,m}^{l}\left(x',\, y' \right) + b_{m}^{l} \end{aligned} $$
</div>

<h4 id="output">Output</h4>
<div class="math-definition">
$$ o_{m}^{l}\left(x,\, y\right) = a\left(c_{m}^{l}\left(x,\, y\right)\right)$$
</div>

<h4 id="error">Error</h4>
<div class="math-definition">
$$ E = \frac{1}{2} \left( t - o^L \right)^{2} $$
</div>

<h3 id="backpropagation">Backpropagation</h3>

<h4 id="weight-update">Weight Update</h4>
<div class="math-definition">
$$
\frac{\partial E}{\partial w_{n,m}^{l}\left(x,\, y\right)} = \sum_{x',y'}\underbrace{\frac{\partial E}{\partial c_{m}^{l}\left(x',\, y'\right)}}_{\delta_{m}^{l}\left(x',\, y' \right)} \cdot \frac{\partial c_{m}^{l}\left(x',\, y'\right)}{\partial w_{n,m}^{l}\left(x,\, y\right)}
$$
</div>

<div class="math-definition">
$$
\begin{aligned}
\frac{\partial c_{m}^{l}\left(x',\, y'\right)}{\partial w_{n,m}^{l}\left(x,\, y\right)} &amp;= \frac{\partial}{\partial w_{n,m}^{l}\left(x,\, y\right)} \left( \sum_{n,\, x'',\, y''} o_{n}^{l-1}\left(x'-x'',\, y'-y''\right) \cdot w_{n,m}^{l}\left(x'',\, y'' \right) + b_{m}^{l} \right) \\
  &amp;= \frac{\partial}{\partial w_{n,m}^{l}\left(x,\, y\right)} \left( o_{0}^{l-1}\left( x'-0,\, y'-0 \right) \cdot w_{0,m}^{l}\left( 0, \, 0 \right) + \ldots + o_{n}^{l-1}\left( x' - x,\, y' - y \right) \cdot w_{n,m}^{l} \left( x,\, y \right) + \ldots + b_{m}^{l} \right) \\
  &amp;= o_{n}^{l-1}\left( x'-x,\, y' - y \right)
\end{aligned}
$$
</div>

<div class="math-definition">
$$
\frac{\partial E}{\partial w_{n,m}^{l}\left(x,\, y\right)} = \sum_{x',y'} o_{n}^{l-1}\left( x'-x,\, y' - y \right) \cdot \delta_{m}^{l}\left(x',\, y' \right)
$$
</div>

<p>\( \left( x’-x,\, y’ - y \right) \) will go out of bounds of \( o_{n}^{l-1} \), 
since \( x’ \) and \( y’ \) run over the smaller \( \delta_{m}^{l} \). We can resolve this by wrapping around the indexes, thus  rewriting \( \left(x’-x,\,y’-y\right) \) to \( \left( N - x + x’,\, N - y + y’ \right)\), where \( N \) is the size of \( o_{n}^{l-1}\). As a result we can apply cross-correlation, when rotating the result by 180°. We can rewrite the weight update as follows:</p>

<div class="math-definition">
$$
\frac{\partial E}{\partial w_{n,m}^{l}\left(x,\, y\right)} = \operatorname{rot}_{180}\left(\underbrace{\sum_{x',y'} o_{n}^{l-1}\left( x + x',\, y + y' \right) \cdot \delta_{m}^{l}\left(x',\, y' \right)}_{\text{Cross-Correlation}}\right)
$$
</div>

<h4 id="deltas">Deltas</h4>
<div class="math-definition">
$$
\delta_{m}^{l}\left(x,\: y\right) = \frac{\partial E}{\partial c_{m}^{l}\left(x,\, y\right)}=\sum_{o,\, x',\, y'}\underbrace{\frac{\partial E}{\partial c_{o}^{l+1}\left(x',\, y'\right)}}_{\delta_{o}^{l+1}\left(x',\, y'\right)}\cdot\frac{\partial c_{o}^{l+1}\left(x',\, y'\right)}{c_{m}^{l}\left(x,\, y\right)}
$$
</div>

<div class="math-definition">
$$
\begin{aligned}

\frac{\partial c_{o}^{l+1}\left(x',\, y'\right)}{\partial c_{m}^{l}\left(x,\, y\right)} &amp;= \frac{\partial}{\partial c_{m}^{l}\left(x,\, y\right)}\left(\sum_{m',\, x'',\, y''}w_{m',o}^{l+1}\left(x'',\, y''\right)\cdot a\left(c_{m'}^{l}\left(x'-x'',\, y'-y''\right)\right)\right) \\
  &amp;= \frac{\partial}{\partial c_{m}^{l}\left(x,\, y\right)}\left(w_{0,o}^{l+1}\left(0,\,0\right)\cdot a\left(c_{0}^{l}\left(x'-0,\, y'-0\right)\right)+\ldots+w_{m,o}^{l+1}\left(x'',\, y''\right)\cdot a\left(c_{m}^{l}\underbrace{\left(x,\, y\right)}_{\left(x' - x'',\, y' - y''\right)}\right)+\ldots\right) \\
  &amp;= w_{m,o}^{l+1}\left( x'-x,\, y'-y \right) \cdot \frac{\partial a\left( c_{m}^{l}\left(x,\, y\right) \right)}{\partial c_{m}^{l}\left(x,\, y\right)}
  
\end{aligned}
$$
</div>

<div class="math-definition">
$$
\delta_{m}^{l}\left(x,\: y\right) = \underbrace{\sum_{o,\, x',\, y'} w_{m,o}^{l+1}\left( x'-x,\, y'-y \right) \cdot \delta_{o}^{l+1}\left( x',\, y' \right)}_{\text{backpropagated error}} \cdot \frac{\partial a\left( c_{m}^{l}\left(x,\, y\right) \right)}{\partial c_{m}^{l}\left(x,\, y\right)}
$$
</div>

<p>Thinking of \( \left(x’-x,\, y’-y \right) \) as \( \left(N-x+x’,\, N-y-y’ \right) \) we can again rewrite the delta recursion as cross-correlation where the result is rotated by 180°:</p>

<div class="math-definition">
$$
\delta_{m}^{l}\left(x,\: y\right) = \operatorname{rot}_{180} \left(\underbrace{\sum_{o,\, x',\, y'} w_{m,o}^{l+1}\left( x+x',\, y+y' \right) \cdot \delta_{o}^{l+1}\left( x',\, y' \right)}_{\text{Cross-Correlation}} \right) \cdot \frac{\partial a\left( c_{m}^{l}\left(x,\, y\right) \right)}{\partial c_{m}^{l}\left(x,\, y\right)}
$$
</div>

<!--<div id="svg-container" class="svg-container"></div>-->

<!--<script type="text/javascript">-->
<!--  mathJaxRendered = function() {-->
<!--    var svg = $("svg");-->
<!--    addMathToSvg("mathJaxSource", svg);-->
<!--  }-->
<!--  -->
<!--  MathJax.Hub.Register.StartupHook("End Typeset", mathJaxRendered);-->
<!--  -->
<!--  var raphael = Raphael("svg-container", '100%', '100%');-->
<!--  raphael.setViewBox(0, 0, 800, 800, true);-->
<!--  -->
<!--  var grid = new Grid(10, 10, 8, 8, 10, 10);-->
<!--  -->
<!--  var neurons = [];-->
<!--  var links = [];-->
<!--  neurons.push(new Node(300, 50, 80, 20));-->
<!--  neurons.push(new Node(300, 150, 80, 20));-->
<!--  neurons.push(new Node(500, 50, 80, 20));-->
<!--  neurons.push(new Node(500, 150, 80, 20));-->
<!--  -->
<!--  links.push(neurons[0].linkTo(neurons[2]));-->
<!--  links.push(neurons[0].linkTo(neurons[3]));-->
<!--  links.push(neurons[1].linkTo(neurons[2]));-->
<!--  links.push(neurons[1].linkTo(neurons[3]));-->
<!--  -->
<!--  // draw-->
<!--  grid.draw(raphael);-->
<!--  for (var i = 0; i < neurons.length; i++) {-->
<!--    neurons[i].draw(raphael);-->
<!--  }-->
<!--  for (var i = 0; i < links.length; i++) {-->
<!--    links[i].draw(raphael);-->
<!--  }-->
<!--</script>-->
<!--<div id="mathJaxSource" style="display: none">-->
<!--  <math xmlns="http://www.w3.org/1998/Math/MathML"><mo>&#8721;</mo><mi>c</mi><mo>+</mo><mi>&#946;</mi></math>-->
<!--</div>-->

</div>
</article>
